{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn import *\n",
    "import re\n",
    "from SimpleCountVectorizer_MAD import *\n",
    "from TFIDFVectorizer import *\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.grammar import ProbabilisticProduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\"\"\"Given an int32 number, print it in English.\"\"\"\n",
    "def int_to_en(num):\n",
    "    d = { 0 : 'zero', 1 : 'one', 2 : 'two', 3 : 'three', 4 : 'four', 5 : 'five',\n",
    "          6 : 'six', 7 : 'seven', 8 : 'eight', 9 : 'nine', 10 : 'ten',\n",
    "          11 : 'eleven', 12 : 'twelve', 13 : 'thirteen', 14 : 'fourteen',\n",
    "          15 : 'fifteen', 16 : 'sixteen', 17 : 'seventeen', 18 : 'eighteen',\n",
    "          19 : 'nineteen', 20 : 'twenty',\n",
    "          30 : 'thirty', 40 : 'forty', 50 : 'fifty', 60 : 'sixty',\n",
    "          70 : 'seventy', 80 : 'eighty', 90 : 'ninety' }\n",
    "    k = 1000\n",
    "    m = k * 1000\n",
    "    b = m * 1000\n",
    "    t = b * 1000\n",
    "\n",
    "    assert(0 <= num)\n",
    "\n",
    "    if (num < 20):\n",
    "        return d[num]\n",
    "\n",
    "    if (num < 100):\n",
    "        if num % 10 == 0: return d[num]\n",
    "        else: return d[num // 10 * 10] + '-' + d[num % 10]\n",
    "\n",
    "    if (num < k):\n",
    "        if num % 100 == 0: return d[num // 100] + ' hundred'\n",
    "        else: return d[num // 100] + ' hundred and ' + int_to_en(num % 100)\n",
    "\n",
    "    if (num < m):\n",
    "        if num % k == 0: return int_to_en(num // k) + ' thousand'\n",
    "        else: return int_to_en(num // k) + ' thousand, ' + int_to_en(num % k)\n",
    "\n",
    "    if (num < b):\n",
    "        if (num % m) == 0: return int_to_en(num // m) + ' million'\n",
    "        else: return int_to_en(num // m) + ' million, ' + int_to_en(num % m)\n",
    "\n",
    "    if (num < t):\n",
    "        if (num % b) == 0: return int_to_en(num // b) + ' billion'\n",
    "        else: return int_to_en(num // b) + ' billion, ' + int_to_en(num % b)\n",
    "\n",
    "    if (num % t == 0): return int_to_en(num // t) + ' trillion'\n",
    "    else: return int_to_en(num // t) + ' trillion, ' + int_to_en(num % t)\n",
    "\n",
    "    raise AssertionError('num is too large: %s' % str(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/quora_train_data.csv\")\n",
    "test_df = pd.read_csv('./input/quora_test_data.csv')\n",
    "\n",
    "# train_df, test_df = sklearn.model_selection.train_test_split(train_df, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 6), (80858, 6))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    # return list(map(lambda x: str(x), all_questions)) # Slower\n",
    "    return [str(s) for s in mylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str'}\n"
     ]
    }
   ],
   "source": [
    "all_questions = cast_list_as_strings(list(train_df.loc[:, 'question1'])+list(train_df.loc[:, 'question2']))\n",
    "print(set(type(x).__name__ for x in all_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def num_conv(s):\n",
    "    try:\n",
    "        return int_to_en(int(s)).replace(\",\",\"\").replace(\" \",\"_\")\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "\n",
    "def my_doc_cleaner(doc,\n",
    "                  pat=r\"[^a-zA-Z0-9]\"):\n",
    "    # Allow alphanumeric characters\n",
    "    doc_cleaner_pattern=pat\n",
    "    clean_doc_pattern = re.compile(doc_cleaner_pattern)\n",
    "    doc_clean = clean_doc_pattern.sub(\" \", doc)\n",
    "    return doc.lower()\n",
    "\n",
    "\n",
    "# stpw = set(stopwords.words(\"english\"))\n",
    "stpw = []\n",
    "\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer_func(doc, \n",
    "                      ngrams=(1,3), \n",
    "                      numbers_to_words=True,\n",
    "                      stop_words=stpw,\n",
    "                      pat=r\"(?u)\\b\\w\\S*\\w*\\b\",\n",
    "                      lem=True,\n",
    "                      stem=True,\n",
    "                      n_tokens=True,\n",
    "                     duplicate_question_words=True):\n",
    "    import pdb\n",
    "    # Split using a patterm\n",
    "#     pat=r\"(?u)\\b\\w\\w+\\b\"\n",
    "#     pat=r\"(?u)\\b\\w\\S*\\w*\\b\"\n",
    "    \n",
    "    token_pattern = re.compile(pat)\n",
    "    lst = token_pattern.findall(doc)\n",
    "    \n",
    "    # Duplicate key_words\n",
    "    if duplicate_question_words:\n",
    "        question_words = ['who','what','when','where','why','how']\n",
    "        lst += [value for value in lst if value in question_words] \n",
    "        \n",
    "    # Transform numbers into words\n",
    "    if numbers_to_words:\n",
    "        lst = list(map(lambda x: num_conv(x), lst))\n",
    "        \n",
    "    # Drop stopwords \n",
    "    lst = list(filter(lambda x : x not in stop_words, lst))\n",
    "    \n",
    "    #Stemmer\n",
    "    if stem:\n",
    "        lst = list(map(lambda x: stemmer.stem(x), lst))\n",
    "    \n",
    "    #Lemmatizer \n",
    "    if lem:\n",
    "        lst = list(map(lambda x: lemmatizer.lemmatize(x), lst))\n",
    "     \n",
    "\n",
    "    \n",
    "    if ngrams==(1,1):\n",
    "        return lst\n",
    "    \n",
    "    # Generate ngrams\n",
    "    lstRet = []\n",
    "    for a in range(ngrams[0], ngrams[1]+1):\n",
    "        if a!=1:\n",
    "            lstRet+=list(zip(*[lst[i:] for i in range(a)]))\n",
    "    \n",
    "\n",
    "    \n",
    "    # N-tokens\n",
    "    if n_tokens:\n",
    "        lst.append(num_conv(str(len(lst))) + 'tokens')\n",
    "        \n",
    "    \n",
    "    \n",
    "    return lstRet if ngrams[0]!=1 else lst+lstRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a6090d8f904ea7bae010fb3a8c8b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=646864), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCountVectorizer(doc_cleaner_func=<function my_doc_cleaner at 0x000001CFA685A438>,\n",
       "                      doc_cleaner_pattern='[^a-zA-Z]',\n",
       "                      dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "                      token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                      tokenizer_func=<function my_tokenizer_func at 0x000001CFC6692288>,\n",
       "                      word_transformer_func=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = SimpleCountVectorizer(\n",
    "    doc_cleaner_func=my_doc_cleaner,\n",
    "    tokenizer_func=my_tokenizer_func\n",
    ")\n",
    "count_vect.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_features_from_df(df, vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    q1 = vectorizer.transform(q1_casted)\n",
    "    q2 = vectorizer.transform(q2_casted)\n",
    "    \n",
    "    X_q1q2 = scipy.sparse.hstack((q1,q2))\n",
    "        \n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'what', 'when', 'where', 'why', 'how']\n",
      "['who', 'what', 'when', 'where', 'why', 'how']\n",
      "['who', 'what', 'when', 'where', 'why', 'how']\n",
      "['who', 'what', 'when', 'where', 'why', 'how']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((323432, 6675538), (323432, 6), (80858, 6), (80858, 6675538))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2 = get_features_from_df(train_df,count_vect)\n",
    "X_te_q1q2  = get_features_from_df(test_df, count_vect)\n",
    "\n",
    "X_tr_q1q2.shape, train_df.shape, test_df.shape, X_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_train = train_df[\"is_duplicate\"].values\n",
    "y_test = test_df['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", verbose=1, max_iter=100)\n",
    "logistic.fit(X_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9963639961413837, 0.8085285314996661)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X_tr_q1q2, y_train), logistic.score(X_te_q1q2, y_test)\n",
    "# original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9978388038289346, 0.8110267382324569)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X_tr_q1q2, y_train), logistic.score(X_te_q1q2, y_test)\n",
    "# original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9963083430210987, 0.8101362883078977)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X_tr_q1q2, y_train), logistic.score(X_te_q1q2, y_test)\n",
    "# added n_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9977305894283807, 0.8116945756758762)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X_tr_q1q2, y_train), logistic.score(X_te_q1q2, y_test)\n",
    "# n_tokens and duplicate_question_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9978388038289346, 0.8110267382324569)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X_tr_q1q2, y_train), logistic.score(X_te_q1q2, y_test)\n",
    "# n_tokens and modified duplicate_question_word (triple question word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\maikito\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\maikito\\anaconda3\\lib\\site-packages (from xgboost) (1.16.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\maikito\\anaconda3\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.60892\ttrain-logloss:0.69073\tvalid-auc:0.60823\tvalid-logloss:0.69080\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[25]\ttrain-auc:0.60901\ttrain-logloss:0.66507\tvalid-auc:0.60830\tvalid-logloss:0.66550\n",
      "[50]\ttrain-auc:0.60901\ttrain-logloss:0.65734\tvalid-auc:0.60830\tvalid-logloss:0.65794\n",
      "[75]\ttrain-auc:0.60901\ttrain-logloss:0.65435\tvalid-auc:0.60830\tvalid-logloss:0.65522\n",
      "[100]\ttrain-auc:0.60901\ttrain-logloss:0.65327\tvalid-auc:0.60830\tvalid-logloss:0.65428\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# PARAM GRID\n",
    "param_grid = {}\n",
    "param_grid['objective'] = 'binary:logistic'\n",
    "param_grid['eval_metric'] = ['auc', 'logloss']   #error: (wrong cases)/#(all cases)\n",
    "param_grid['eta'] = 0.02\n",
    "param_grid['max_depth'] = 10\n",
    "param_grid['n_estimators'] = 300\n",
    "\n",
    "d_train = xgb.DMatrix(X_tr_q1q2, label=y_train)  # For sparse matrices\n",
    "d_valid = xgb.DMatrix(X_te_q1q2, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(param_grid, d_train, 300, watchlist, early_stopping_rounds=50, verbose_eval=25, )\n",
    "#300 # train-auc:0.818362\ttrain-logloss:0.527855\tvalid-auc:0.801083\tvalid-logloss:0.53815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_mistakes(clf, X_q1q2, y):\n",
    "\n",
    "    predictions = clf.predict(X_q1q2)    \n",
    "    incorrect_predictions = predictions!=y\n",
    "\n",
    "    incorrect_indices = np.where(incorrect_predictions)[0]\n",
    "    \n",
    "    if np.sum(incorrect_predictions)==0:\n",
    "        print(\"no mistakes in this df\")\n",
    "    else:\n",
    "        return incorrect_indices, predictions\n",
    "    \n",
    "def print_mistake_k(k, mistake_indices, predictions):\n",
    "    print(train_df.iloc[mistake_indices[k]].question1)\n",
    "    print(train_df.iloc[mistake_indices[k]].question2)\n",
    "    print(\"true class:\", train_df.iloc[mistake_indices[k]].is_duplicate)\n",
    "    print(\"prediction:\", predictions[mistake_indices[k]])\n",
    "    \n",
    "def print_mistake_k_and_tokens(k, mistake_indices, predictions,\n",
    "                               X_q1q2, count_vect, clf):\n",
    "    q1 = train_df.iloc[mistake_indices[k]].question1\n",
    "    q2 = train_df.iloc[mistake_indices[k]].question2\n",
    "    \n",
    "    print(q1)\n",
    "    print(count_vect.tokenize(q1))\n",
    "    print()\n",
    "    print(q2)\n",
    "    print(count_vect.tokenize(q2))\n",
    "    print()\n",
    "    print(\"true class:\", train_df.iloc[mistake_indices[k]].is_duplicate)\n",
    "    print(\"prediction:\", predictions[mistake_indices[k]])\n",
    "    print()\n",
    "    print(\"Probability vector: [P(0|x), P(1|x)]:\")\n",
    "    print(clf.predict_proba(X_q1q2)[mistake_indices[k],:])\n",
    "    \n",
    "    \n",
    "def hist_errors(mistake_indices, predictions,\n",
    "                               X_q1q2, count_vect, clf):\n",
    "    qs = train_df.iloc[mistake_indices][['question1', 'question2']]\n",
    "    qs['true_class']=train_df.iloc[mistake_indices].is_duplicate\n",
    "    qs['prediction']=predictions[mistake_indices]\n",
    "    qs['P(1|x)']=clf.predict_proba(X_q1q2)[mistake_indices,:][:,1]\n",
    "    qs = qs.reset_index(drop=True)\n",
    "    return qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_te_q1q2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I study for Honeywell company recruitment?\n",
      "How do I study for Honeywell company recruitments?\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "print_mistake_k(0, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I study for Honeywell company recruitment?\n",
      "['how', 'do', 'i', 'studi', 'for', 'honeywel', 'compani', 'recruit', ('how', 'do'), ('do', 'i'), ('i', 'studi'), ('studi', 'for'), ('for', 'honeywel'), ('honeywel', 'compani'), ('compani', 'recruit'), ('how', 'do', 'i'), ('do', 'i', 'studi'), ('i', 'studi', 'for'), ('studi', 'for', 'honeywel'), ('for', 'honeywel', 'compani'), ('honeywel', 'compani', 'recruit')]\n",
      "\n",
      "How do I study for Honeywell company recruitments?\n",
      "['how', 'do', 'i', 'studi', 'for', 'honeywel', 'compani', 'recruit', ('how', 'do'), ('do', 'i'), ('i', 'studi'), ('studi', 'for'), ('for', 'honeywel'), ('honeywel', 'compani'), ('compani', 'recruit'), ('how', 'do', 'i'), ('do', 'i', 'studi'), ('i', 'studi', 'for'), ('studi', 'for', 'honeywel'), ('for', 'honeywel', 'compani'), ('honeywel', 'compani', 'recruit')]\n",
      "\n",
      "true class: 1\n",
      "prediction: 0\n",
      "\n",
      "Probability vector: [P(0|x), P(1|x)]:\n",
      "[0.81332746 0.18667254]\n"
     ]
    }
   ],
   "source": [
    "print_mistake_k_and_tokens(0, mistake_indices, predictions,\n",
    "                           X_te_q1q2, count_vect, logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TFIDFVectorizer(count_vect.vocabulary, count_vect.word_to_ind, count_vect.tokenize)\n",
    "tfidf_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_tfidf_tr_q1q2 = get_features_from_df(train_df, tfidf_vectorizer)\n",
    "X_tfidf_te_q1q2  = get_features_from_df(test_df, tfidf_vectorizer)\n",
    "\n",
    "X_tfidf_tr_q1q2.shape, train_df.shape, test_df.shape, X_tfidf_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", verbose=1, max_iter=100)\n",
    "logistic.fit(X_tfidf_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logistic.score(X_tfidf_tr_q1q2, y_train), logistic.score(X_tfidf_te_q1q2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tfidf_te_q1q2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print_mistake_k(0, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print_mistake_k_and_tokens(0, mistake_indices, predictions,\n",
    "                           X_tfidf_te_q1q2, tfidf_vectorizer, logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "######### SEARCH QUESTIONS WITH SAME TOKENS ###########\n",
    "\n",
    "q1_casted =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_casted =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "\n",
    "q1 = count_vect.transform(q1_casted)\n",
    "q2 = count_vect.transform(q2_casted)\n",
    "\n",
    "same_tokens_idxs = []\n",
    "for i in range(len(q1_casted)):\n",
    "    same_features = ( q1[i] != q2[i] ).nnz == 0\n",
    "    if same_features:\n",
    "        same_tokens_idxs.append(i)\n",
    "    if i % 500 == 0: print(i)\n",
    "\n",
    "same_tokens_idxs = np.array(same_tokens_idxs)\n",
    "\n",
    "\n",
    "same_tokens = train_df.iloc[same_tokens_idxs]\n",
    "duplicates = same_tokens['is_duplicate'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "same_tokens[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "same_tokens[~duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
