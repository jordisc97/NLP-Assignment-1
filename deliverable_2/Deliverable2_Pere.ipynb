{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2\n",
    "\n",
    "- Deliverable 2 will be a NER (Named entity recognition system).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the data\n",
    "\n",
    "url = https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
    "\n",
    "\n",
    "Essential info about entities:\n",
    "\n",
    "```\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:43:11.016579Z",
     "start_time": "2020-05-05T15:43:11.012243Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:43:22.865464Z",
     "start_time": "2020-05-05T15:43:22.482990Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path+\"ner_dataset.csv\",\n",
    "                   encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:43:50.431431Z",
     "start_time": "2020-05-05T15:43:50.418337Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fill with \"Sentence: k\" for each k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:24.000309Z",
     "start_time": "2020-05-05T14:07:23.916878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47960"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list(set(data[\"Sentence #\"]))\n",
    "sentences[0] = \"nan\"\n",
    "sentences.sort()\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:24.866376Z",
     "start_time": "2020-05-05T14:07:24.861661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence: 1', 'Sentence: 10', 'Sentence: 100']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:25.213336Z",
     "start_time": "2020-05-05T14:07:25.153708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data[\"Tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:26.407180Z",
     "start_time": "2020-05-05T14:07:25.694653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TAG: I-per\n",
      "271         Mahmoud\n",
      "272     Ahmadinejad\n",
      "332         Horbach\n",
      "444       Abdullahi\n",
      "445           Yusuf\n",
      "446           Ahmad\n",
      "966        Muhammad\n",
      "974          Khayam\n",
      "1106     Faridullah\n",
      "1107           Khan\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: I-eve\n",
      "4854      Summer\n",
      "4855    Olympics\n",
      "5036     Olympic\n",
      "5171      Medusa\n",
      "5764         War\n",
      "6730        Open\n",
      "6756     Classic\n",
      "6834        Open\n",
      "9990         War\n",
      "9991          II\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-tim\n",
      "167    Wednesday\n",
      "211    Wednesday\n",
      "274      Tuesday\n",
      "341    Wednesday\n",
      "493    Wednesday\n",
      "654       Sunday\n",
      "679     Saturday\n",
      "684       Friday\n",
      "740     Saturday\n",
      "848     Thursday\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: I-nat\n",
      "5045            Jing\n",
      "5074            Jing\n",
      "12509          Acute\n",
      "12510    Respiratory\n",
      "12511       Syndrome\n",
      "22948        Katrina\n",
      "23055        Katrina\n",
      "29719        Katrina\n",
      "34813        Katrina\n",
      "68389        Katrina\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-geo\n",
      "6        London\n",
      "12         Iraq\n",
      "65         Hyde\n",
      "94      Britain\n",
      "106    Brighton\n",
      "118        Iraq\n",
      "133      London\n",
      "146        Rome\n",
      "148       Paris\n",
      "151      Madrid\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: I-geo\n",
      "66            Park\n",
      "347          State\n",
      "350          State\n",
      "381          Delta\n",
      "561           Arab\n",
      "796           West\n",
      "797       Frontier\n",
      "798       Province\n",
      "1112    Waziristan\n",
      "1122           Wam\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-art\n",
      "263       Nuclear\n",
      "3769     Saltillo\n",
      "3810    Pentastar\n",
      "3814     Chrysler\n",
      "3816        Dodge\n",
      "3818         Jeep\n",
      "3820          Ram\n",
      "3863        Vioxx\n",
      "3951        Vioxx\n",
      "3962        Vioxx\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-org\n",
      "97             Labor\n",
      "154    International\n",
      "215             IAEA\n",
      "234         European\n",
      "248             U.N.\n",
      "328        Bilfinger\n",
      "359      Royal-Dutch\n",
      "370            Shell\n",
      "543               al\n",
      "597               al\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: I-org\n",
      "98        Party\n",
      "155      Atomic\n",
      "156      Energy\n",
      "157      Agency\n",
      "235       Union\n",
      "249    Security\n",
      "250     Council\n",
      "329      Berger\n",
      "360       Shell\n",
      "544       Qaida\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: I-art\n",
      "264     Non-Proliferation\n",
      "3811                  V-6\n",
      "4016               Simple\n",
      "4017                 Life\n",
      "4142              Morning\n",
      "4143              America\n",
      "5248               Mirror\n",
      "5923                   De\n",
      "5924               Gaulle\n",
      "5935        International\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-gpe\n",
      "18     British\n",
      "102    English\n",
      "113    Britain\n",
      "126    British\n",
      "173       Iran\n",
      "181       Iran\n",
      "196    Iranian\n",
      "238       U.S.\n",
      "245       Iran\n",
      "259     Tehran\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: I-tim\n",
      "1479       8\n",
      "1993       1\n",
      "4137       2\n",
      "4148       3\n",
      "4979      of\n",
      "4980    2005\n",
      "7896      25\n",
      "7897       ,\n",
      "7898    1995\n",
      "8254       7\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: O\n",
      "0         Thousands\n",
      "1                of\n",
      "2     demonstrators\n",
      "3              have\n",
      "4           marched\n",
      "5           through\n",
      "7                to\n",
      "8           protest\n",
      "9               the\n",
      "10              war\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-eve\n",
      "4853          2012\n",
      "4887         Games\n",
      "5001         Games\n",
      "5035          2008\n",
      "5170     Operation\n",
      "5763          Gulf\n",
      "6729    Australian\n",
      "6755       Kooyong\n",
      "6833    Australian\n",
      "9989         World\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-per\n",
      "42           Bush\n",
      "270     President\n",
      "331        Thomas\n",
      "443     President\n",
      "965       Prophet\n",
      "973          Omar\n",
      "997        Khayam\n",
      "1055       Khayam\n",
      "1105        Malik\n",
      "1240        Abdul\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: B-nat\n",
      "2723       H5N1\n",
      "4554       H5N1\n",
      "5044       Jing\n",
      "5073       Jing\n",
      "5606       H5N1\n",
      "12506      SARS\n",
      "12508    Severe\n",
      "13162       HIV\n",
      "13164      AIDS\n",
      "22260      AIDS\n",
      "Name: Word, dtype: object\n",
      "\n",
      "TAG: I-gpe\n",
      "1225    States\n",
      "1264     Korea\n",
      "2713      Binh\n",
      "2932     Ababa\n",
      "3466      City\n",
      "5241     Lanka\n",
      "5313     Korea\n",
      "5361     Korea\n",
      "5370     Korea\n",
      "5390     Korea\n",
      "Name: Word, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for tag in set(data[\"Tag\"]):\n",
    "    print(\"\\nTAG:\",tag)\n",
    "    print(data[data[\"Tag\"] == tag][\"Word\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:42.675734Z",
     "start_time": "2020-05-05T14:07:42.672591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_formatter = \"Sentence: {}\"\n",
    "sentence_formatter.format(0) in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:44.193383Z",
     "start_time": "2020-05-05T14:07:44.189951Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_formatter = \"Sentence: {}\"\n",
    "sentence_formatter.format(1) in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:44.474063Z",
     "start_time": "2020-05-05T14:07:44.471021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sentence: 1', 'Sentence: 2')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "sentence_id      = sentence_formatter.format(i)\n",
    "sentence_id_next = sentence_formatter.format(i+1)\n",
    "sentence_id, sentence_id_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:44.798669Z",
     "start_time": "2020-05-05T14:07:44.764080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([0], dtype='int64')\n",
      "Int64Index([24], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(data.index[data[\"Sentence #\"] == sentence_id])\n",
    "print(data.index[data[\"Sentence #\"] == sentence_id_next])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:46.813348Z",
     "start_time": "2020-05-05T14:07:46.776735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 24)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = data.index[data[\"Sentence #\"] == sentence_id][0]\n",
    "end   =  data.index[data[\"Sentence #\"] == sentence_id_next][0]\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:47.090322Z",
     "start_time": "2020-05-05T14:07:47.069311Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"Sentence #\"][start:end] = sentence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:47.432914Z",
     "start_time": "2020-05-05T14:07:47.429106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Sentence: 1\n",
       "1     Sentence: 1\n",
       "2     Sentence: 1\n",
       "3     Sentence: 1\n",
       "4     Sentence: 1\n",
       "5     Sentence: 1\n",
       "6     Sentence: 1\n",
       "7     Sentence: 1\n",
       "8     Sentence: 1\n",
       "9     Sentence: 1\n",
       "10    Sentence: 1\n",
       "11    Sentence: 1\n",
       "12    Sentence: 1\n",
       "13    Sentence: 1\n",
       "14    Sentence: 1\n",
       "15    Sentence: 1\n",
       "16    Sentence: 1\n",
       "17    Sentence: 1\n",
       "18    Sentence: 1\n",
       "19    Sentence: 1\n",
       "20    Sentence: 1\n",
       "21    Sentence: 1\n",
       "22    Sentence: 1\n",
       "23    Sentence: 1\n",
       "Name: Sentence #, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentence #\"][start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a subset and writting an identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:54.788621Z",
     "start_time": "2020-05-05T14:07:54.427362Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path+\"ner_dataset.csv\",\n",
    "                   encoding=\"latin1\")\n",
    "\n",
    "last_n = 2000\n",
    "end   = data.index[data[\"Sentence #\"] == sentence_formatter.format(last_n)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:55.351782Z",
     "start_time": "2020-05-05T14:07:55.348813Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[0:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:07:55.629433Z",
     "start_time": "2020-05-05T14:07:55.619897Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "n_sentences = len(list(set(data[\"Sentence #\"])))\n",
    "first_n = 1\n",
    "last_n = last_n -1\n",
    "print(n_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:07.569498Z",
     "start_time": "2020-05-05T14:07:59.107346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e5d0c7dc82450797296fcbf2c259f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1998.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 13.2 s, sys: 280 ms, total: 13.4 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "# %%time \n",
    "sentence_formatter = \"Sentence: {}\"\n",
    "\n",
    "for s_id in tqdm(range(first_n, last_n)):\n",
    "#     print(\"current {}/{}\".format(s_id,last_n), end=\"\\r\")\n",
    "    sentence_id = sentence_formatter.format(s_id)\n",
    "    sentence_id_next = sentence_formatter.format(s_id + 1)\n",
    "    start = data.index[data[\"Sentence #\"] == sentence_id][0]\n",
    "    end   = data.index[data[\"Sentence #\"] == sentence_id_next][0]\n",
    "    data[\"Sentence #\"][start:end] = sentence_id\n",
    "    \n",
    "sentence_id = sentence_formatter.format(last_n)\n",
    "start = data.index[data[\"Sentence #\"] == sentence_id][0]\n",
    "end   = data.shape[0]\n",
    "data[\"Sentence #\"][start:end] = sentence_id\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: Replace all this previous stuff with:\n",
    "# ffill replaces NaN values with the last non-Nan value -> it fills the ['Sentence #'] column with the correct sentence id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv(path+\"ner_dataset.csv\",\n",
    "                   encoding=\"latin1\").fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:07.574107Z",
     "start_time": "2020-05-05T14:08:07.570959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:15.562269Z",
     "start_time": "2020-05-05T14:08:07.575850Z"
    }
   },
   "outputs": [],
   "source": [
    "X_txt = []\n",
    "Y_txt = []\n",
    "\n",
    "sentence_formatter = \"Sentence: {}\"\n",
    "\n",
    "for i in range(1,n_sentences):\n",
    "    s = sentence_formatter.format(i)\n",
    "    X_txt.append(list(data[data[\"Sentence #\"]==s][\"Word\"].values))\n",
    "    Y_txt.append(list(data[data[\"Sentence #\"]==s][\"Tag\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:15.567398Z",
     "start_time": "2020-05-05T14:08:15.563916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands/O of/O demonstrators/O have/O marched/O through/O London/B-geo to/O protest/O the/O war/O in/O Iraq/B-geo and/O demand/O the/O withdrawal/O of/O British/B-gpe troops/O from/O that/O country/O ./O'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "xy = [\"{}/{}\".format(x,y) for x,y in zip(X_txt[i],Y_txt[i])]\n",
    "\" \".join(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:15.573987Z",
     "start_time": "2020-05-05T14:08:15.569071Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_word_to_pos(X):\n",
    "\n",
    "    word_to_pos = {}\n",
    "    i = 0\n",
    "    for s in X:\n",
    "        for w in s:\n",
    "            if w not in word_to_pos:\n",
    "                word_to_pos[w] = i\n",
    "                i +=1\n",
    "                \n",
    "    pos_to_word = {v: k for k, v in word_to_pos.items()}\n",
    "    return word_to_pos, pos_to_word\n",
    "            \n",
    "def build_tag_to_pos(Y):\n",
    "    tag_to_pos = {}\n",
    "    i = 0\n",
    "    for s in Y:\n",
    "        for t in s:\n",
    "            if t not in tag_to_pos:\n",
    "                tag_to_pos[t] = i\n",
    "                i +=1\n",
    "    pos_to_tag = {v: k for k, v in tag_to_pos.items()}\n",
    "\n",
    "    return tag_to_pos, pos_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:15.584465Z",
     "start_time": "2020-05-05T14:08:15.575588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7047, 17)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_pos, pos_to_word = build_word_to_pos(X_txt)\n",
    "tag_to_pos, pos_to_tag  = build_tag_to_pos(Y_txt)\n",
    "\n",
    "len(word_to_pos), len(tag_to_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:15.590178Z",
     "start_time": "2020-05-05T14:08:15.586718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T14:08:15.603562Z",
     "start_time": "2020-05-05T14:08:15.592209Z"
    }
   },
   "outputs": [],
   "source": [
    "X = [[word_to_pos[w] for w in s] for s in X_txt]\n",
    "Y = [[tag_to_pos[t] for t in s] for s in Y_txt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "import skseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skseq\n",
    "import skseq.sequences\n",
    "import skseq.readers\n",
    "\n",
    "from skseq.sequences import sequence\n",
    "from skseq.sequences import sequence_list\n",
    "from skseq.sequences import label_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label dictionary containing all the words \n",
    "x_dict = label_dictionary.LabelDictionary(label_names=list(word_to_pos.keys()))\n",
    "\n",
    "# Label dictionary containing all the tags\n",
    "y_dict = label_dictionary.LabelDictionary(label_names=list(tag_to_pos.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sequences\n",
    "train_seq = sequence_list.SequenceList(x_dict,y_dict)\n",
    "train_seq.seq_list = [sequence.Sequence(x,y) for x,y in zip(X_train,Y_train)]\n",
    "\n",
    "test_seq = sequence_list.SequenceList(x_dict,y_dict)\n",
    "test_seq.seq_list = [sequence.Sequence(x,y) for x,y in zip(X_test,Y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/0 5672/0 2194/3 509/0 352/0 13/0 2554/2 1189/0 19/0 9/0 5673/0 369/0 279/0 994/0 107/7 53/0 882/0 1070/0 11/0 9/0 813/0 832/0 1/0 5674/1 21/0  \n",
      "\n",
      " The/O 85-year-old/O Obama/B-per told/O police/O and/O Kenyan/B-gpe media/O that/O the/O break-in/O attempt/O occurred/O early/O Wednesday/B-tim at/O her/O home/O in/O the/O western/O village/O of/O Kogelo/B-geo ./O \n"
     ]
    }
   ],
   "source": [
    "# Decodification of train sentence 0\n",
    "print(train_seq[0],'\\n\\n',train_seq[0].to_words(sequence_list=train_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/0 1171/0 18/0 9/0 1384/0 1569/0 172/0 9/0 1566/0 68/0 1570/0 1571/0 1069/0 11/0 247/0 1572/0 21/0  \n",
      "\n",
      " A/O statement/O from/O the/O health/O ministry/O said/O the/O woman/O 's/O family/O kept/O chickens/O in/O their/O backyard/O ./O \n"
     ]
    }
   ],
   "source": [
    "# Decodification of train sentence 0\n",
    "print(test_seq[0], '\\n\\n', test_seq[0].to_words(sequence_list=test_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From notebook 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_initial_counts(initial_counts, seq, state_to_pos):\n",
    "    initial_counts[state_to_pos[seq.y[0]]] +=  1\n",
    "\n",
    "def update_transition_counts(transition_counts, seq, state_to_pos):\n",
    "    for (t_prev,t) in zip(seq.y[:-1], seq.y[1:]):\n",
    "        transition_counts[state_to_pos[t], state_to_pos[t_prev]] += 1 \n",
    "\n",
    "def update_emission_counts(emission_counts, seq, state_to_pos, word_to_pos):\n",
    "    for (t,w) in zip(seq.y, seq.x):\n",
    "        emission_counts[state_to_pos[t], word_to_pos[w]] += 1 \n",
    "        \n",
    "def update_final_counts(final_counts, seq, state_to_pos):\n",
    "    final_counts[state_to_pos[seq.y[-1]]] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sufficient_statistics_hmm(sequences, state_to_pos, word_to_pos):\n",
    "    \n",
    "    n_states = len(state_to_pos)\n",
    "    n_words  = len(word_to_pos)\n",
    "    initial_counts      = np.zeros((n_states))\n",
    "    transition_counts   = np.zeros((n_states, n_states))\n",
    "    final_counts        = np.zeros((n_states))\n",
    "    emission_counts     = np.zeros((n_states, n_words))\n",
    "    \n",
    "    for seq in sequences:\n",
    "        update_initial_counts(initial_counts, seq, state_to_pos)\n",
    "        update_transition_counts(transition_counts, seq,  state_to_pos)\n",
    "        update_emission_counts(emission_counts, seq,  state_to_pos, word_to_pos) \n",
    "        update_final_counts(final_counts, seq,  state_to_pos) \n",
    "    \n",
    "    return initial_counts, transition_counts, final_counts, emission_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt, X_test_txt, Y_train_txt, Y_test_txt = train_test_split(X_txt, Y_txt, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_txt = [sequence.Sequence(x,y) for x,y in zip(X_train_txt,Y_train_txt)]\n",
    "test_seq_txt = [sequence.Sequence(x,y) for x,y in zip(X_test_txt,Y_test_txt)]\n",
    "all_seq_txt = [sequence.Sequence(x,y) for x,y in zip(X_txt,Y_txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sufficient_statistics_hmm(all_seq_txt, \n",
    "                                   tag_to_pos,\n",
    "                                   word_to_pos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_counts, transition_counts, final_counts, emission_counts = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\t 1999 \n",
      "N:\t 24 \n",
      "M*N:\t 47976\n"
     ]
    }
   ],
   "source": [
    "M = len(all_seq_txt)\n",
    "N = len(all_seq_txt[0].x)\n",
    "print(\"M:\\t\", M, \"\\nN:\\t\", N,\"\\nM*N:\\t\", M*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_counts sum:  1999.0\n",
      "emission_counts sum:  44375.0\n",
      "transition and final counts sum:  44375.0\n"
     ]
    }
   ],
   "source": [
    "print(\"initial_counts sum: \", np.sum(initial_counts))\n",
    "print(\"emission_counts sum: \", np.sum(emission_counts))\n",
    "print(\"transition and final counts sum: \",\\\n",
    "       np.sum(transition_counts) + sum(final_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "initial_probs\n",
      "[7.19859930e-01 4.60230115e-02 8.70435218e-02 9.00450225e-02\n",
      " 0.00000000e+00 4.75237619e-02 0.00000000e+00 9.00450225e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 5.00250125e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00]\n",
      "\n",
      "transition_probs\n",
      "[[8.45657293e-01 7.93157076e-01 8.64253394e-01 1.99712644e-01\n",
      "  8.83534137e-01 5.69794050e-01 5.48701299e-01 8.04941482e-01\n",
      "  6.73913043e-01 5.20000000e-01 6.67901235e-01 8.33333333e-01\n",
      "  6.63461538e-01 6.11111111e-01 4.06250000e-01 7.50000000e-01\n",
      "  7.77777778e-01]\n",
      " [3.14029472e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.14416476e-03 0.00000000e+00 3.90117035e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.33333333e-02\n",
      "  9.61538462e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.83073626e-02 5.44323484e-03 0.00000000e+00 5.74712644e-03\n",
      "  0.00000000e+00 1.14416476e-03 3.24675325e-03 3.90117035e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.80769231e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.12172280e-02 7.77604977e-04 7.80542986e-02 0.00000000e+00\n",
      "  0.00000000e+00 1.02974828e-02 1.94805195e-02 1.30039012e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.74183515e-01 0.00000000e+00 0.00000000e+00\n",
      "  1.00401606e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.98417948e-02 0.00000000e+00 2.71493213e-02 5.74712644e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.30039012e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.10755149e-01 4.17207792e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.83602741e-02 2.48833593e-02 1.13122172e-03 0.00000000e+00\n",
      "  1.60642570e-02 3.43249428e-03 1.13636364e-02 0.00000000e+00\n",
      "  2.17391304e-02 4.00000000e-02 8.64197531e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.12500000e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.16405196e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.28832952e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.04347826e-01 4.40000000e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 7.88793103e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.22222222e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.94117647e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.33333333e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.83355007e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.22115385e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.23291621e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.30039012e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [8.46583243e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 5.62500000e-01 2.50000000e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.88888889e-01 0.00000000e+00 0.00000000e+00\n",
      "  2.22222222e-01]]\n",
      "\n",
      "final_probs\n",
      "[0.05277917 0.00155521 0.         0.         0.         0.00114416\n",
      " 0.         0.         0.         0.         0.00123457 0.\n",
      " 0.         0.         0.         0.         0.        ]\n",
      "\n",
      "emission_probs\n",
      "[[1.85190084e-04 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.93658562e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.58734358e-04 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [2.64557263e-05 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.64557263e-05 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.55520995e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "initial_probs    = (initial_counts / np.sum(initial_counts))\n",
    "transition_probs = transition_counts/(np.sum(transition_counts,0) + final_counts)\n",
    "final_probs      = final_counts/(np.sum(transition_counts, 0) + final_counts )\n",
    "emission_probs   = emission_counts.T / np.sum(emission_counts, 1)\n",
    "\n",
    "print(\"\\ninitial_probs\")\n",
    "print(initial_probs)\n",
    "\n",
    "print(\"\\ntransition_probs\")\n",
    "print(transition_probs)\n",
    "\n",
    "print(\"\\nfinal_probs\")\n",
    "print(final_probs)\n",
    "\n",
    "print(\"\\nemission_probs\")\n",
    "print(emission_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_with_final_counts = np.vstack((transition_counts,\n",
    "                                           final_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logzero():\n",
    "    return -np.inf\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "    print(x)\n",
    "    if x == 0:\n",
    "        return logzero()\n",
    "    return np.log(x)\n",
    "\n",
    "\n",
    "def logsum_pair(logx, logy):\n",
    "    \"\"\"\n",
    "    Return log(x+y), avoiding arithmetic underflow/overflow.\n",
    "\n",
    "    logx: log(x)\n",
    "    logy: log(y)\n",
    "\n",
    "    Rationale:\n",
    "\n",
    "    x + y    = e^logx + e^logy\n",
    "             = e^logx (1 + e^(logy-logx))\n",
    "    log(x+y) = logx + log(1 + e^(logy-logx)) (1)\n",
    "\n",
    "    Likewise,\n",
    "    log(x+y) = logy + log(1 + e^(logx-logy)) (2)\n",
    "\n",
    "    The computation of the exponential overflows earlier and is less precise\n",
    "    for big values than for small values. Due to the presence of logy-logx\n",
    "    (resp. logx-logy), (1) is preferred when logx > logy and (2) is preferred\n",
    "    otherwise.\n",
    "    \"\"\"\n",
    "    if logx == logzero():\n",
    "        return logy\n",
    "    elif logx > logy:\n",
    "        return logx + np.log1p(np.exp(logy-logx))\n",
    "    else:\n",
    "        return logy + np.log1p(np.exp(logx-logy))\n",
    "\n",
    "\n",
    "def logsum(logv):\n",
    "    \"\"\"\n",
    "    Return log(v[0]+v[1]+...), avoiding arithmetic underflow/overflow.\n",
    "    \"\"\"\n",
    "    res = logzero()\n",
    "    for val in logv:\n",
    "        res = logsum_pair(res, val)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \n",
    "    def __init__(self, word_to_pos={}, state_to_pos={}):\n",
    "        self.fitted = False\n",
    "        self.counts = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.probs  = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.scores = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.decode = set([\"posterior\", \"viterbi\"])\n",
    "        self.word_to_pos  = word_to_pos\n",
    "        self.state_to_pos = state_to_pos\n",
    "        self.pos_to_word  = {v: k for k, v in word_to_pos.items()}\n",
    "        self.pos_to_state = {v: k for k, v in state_to_pos.items()}\n",
    "    \n",
    "        self.n_states     = len(state_to_pos)\n",
    "        self.n_words      = len(word_to_pos)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, observation_lables: list, state_labels: list):\n",
    "        \"\"\"\n",
    "        Computes and saves: counts, probs, scores.\n",
    "        \"\"\"\n",
    "        if self.state_to_pos is None or self.word_to_pos is None:\n",
    "            print(\"Error state_to_pos or word_to_pos needed to be defined\")\n",
    "            return\n",
    "            \n",
    "        self.counts = self.sufficient_statistics_hmm(observation_lables, state_labels)       \n",
    "        self.probs  = self.compute_probs(self.counts)  \n",
    "        self.scores = self.compute_scores(self.probs)  \n",
    "        self.fitted = True\n",
    "        \n",
    "    def sufficient_statistics_hmm(self, observation_lables, state_labels):\n",
    "\n",
    "        state_to_pos, word_to_pos = self.state_to_pos, self.word_to_pos\n",
    "        \n",
    "        def update_initial_counts(initial_counts, seq_x, state_to_pos):\n",
    "            initial_counts[state_to_pos[seq_x[0]]] +=  1\n",
    "            \n",
    "        def update_transition_counts(transition_counts, seq_y, state_to_pos):\n",
    "            for (t_prev, t) in zip(seq_y[:-1], seq_y[1:]):\n",
    "                transition_counts[state_to_pos[t], state_to_pos[t_prev]] += 1 \n",
    "\n",
    "        def update_emission_counts(emission_counts, seq_x, seq_y, state_to_pos, word_to_pos):\n",
    "            for (t,x) in zip(seq_y, seq_x):\n",
    "                emission_counts[state_to_pos[t], word_to_pos[x]] += 1 \n",
    "                \n",
    "        def update_final_counts(final_counts, seq_y, state_to_pos):\n",
    "            final_counts[state_to_pos[seq_y[-1]]] +=1\n",
    "\n",
    "        n_states = len(state_to_pos)\n",
    "        n_words  = len(word_to_pos)\n",
    "        initial_counts      = np.zeros((n_states))\n",
    "        transition_counts   = np.zeros((n_states, n_states))\n",
    "        final_counts        = np.zeros((n_states))\n",
    "        emission_counts     = np.zeros((n_states, n_words))\n",
    "\n",
    "        for seq_x, seq_y in zip(observation_lables, state_labels):\n",
    "            update_initial_counts(initial_counts, seq_y, state_to_pos)\n",
    "            update_transition_counts(transition_counts, seq_y,  state_to_pos)\n",
    "            update_emission_counts(emission_counts, seq_x, seq_y, state_to_pos, word_to_pos) \n",
    "            update_final_counts(final_counts, seq_y,  state_to_pos) \n",
    "\n",
    "        return {\"emission\":   emission_counts, \n",
    "                \"transition\": transition_counts,\n",
    "                \"final\":      final_counts, \n",
    "                \"initial\":    initial_counts}\n",
    "    \n",
    "    def compute_probs(self, counts):\n",
    "        \n",
    "        initial_counts    = counts['initial']\n",
    "        transition_counts = counts['transition']\n",
    "        emission_counts   = counts['emission']\n",
    "        final_counts      = counts['final']\n",
    "\n",
    "        initial_probs    = (initial_counts / np.sum(initial_counts))\n",
    "        transition_probs = transition_counts/(np.sum(transition_counts,0) + final_counts)\n",
    "        final_probs      = final_counts/(np.sum(transition_counts, 0) + final_counts )\n",
    "        emission_probs   = (emission_counts.T / np.sum(emission_counts, 1)).T\n",
    "    \n",
    "        return {\"emission\":   emission_probs, \n",
    "                \"transition\": transition_probs,\n",
    "                \"final\":      final_probs, \n",
    "                \"initial\":    initial_probs}\n",
    "    \n",
    "    def compute_scores(self, probs):\n",
    "         return {\"emission\":   np.log(probs[\"emission\"]), \n",
    "                 \"transition\": np.log(probs[\"transition\"]),\n",
    "                 \"final\":      np.log(probs[\"final\"]), \n",
    "                 \"initial\":    np.log(probs[\"initial\"])}\n",
    "        \n",
    "    def forward_computations(self, x: list):\n",
    "        forward_x = None\n",
    "        return forward_x\n",
    "    \n",
    "    def backward_computations(self, x:list):\n",
    "        backward_x = None\n",
    "        return backward_x\n",
    "    \n",
    "    def log_forward_computations(self, x: list):\n",
    "        \"\"\"\n",
    "        Compute the log_forward computations\n",
    "\n",
    "        Assume there are S possible states and a sequence of length N.\n",
    "        This method will compute iteritavely the log_forward quantities.\n",
    "\n",
    "        * log_f is a S x N Array.\n",
    "        * log_f_x[:,i] will contain the forward quantities at position i.\n",
    "        * log_f_x[:,i] is a vector of size S.\n",
    "        \n",
    "        Returns\n",
    "        - log_f_x: Array of size K x N\n",
    "        \"\"\" \n",
    "        n_x = len(x)\n",
    "        \n",
    "        # log_f_x initialized to -Inf because log(0) = -Inf\n",
    "        log_f_x = np.zeros((self.n_states, n_x)) - np.Inf\n",
    "        x_emission_scores = np.array([hmm.scores['emission'][:, hmm.word_to_pos[w]] for w in x]).T\n",
    "        \n",
    "        log_f_x[:,0] = x_emission_scores[:, 0] + self.scores['initial']\n",
    "        for n in range(1, n_x):\n",
    "            for s in range(self.n_states):\n",
    "                log_f_x[s,n] = logsum(log_f_x[:,n-1] + self.scores['transition'][s,:]) + x_emission_scores[s,n]\n",
    "\n",
    "        log_likelihood = logsum(log_f_x[:,n_x-1] + self.scores['final']) \n",
    "        return log_f_x, log_likelihood # log(P(X=x))\n",
    "    \n",
    "    \n",
    "    def log_backward_computations(self, x: list):\n",
    "        n_x = len(x)\n",
    "        \n",
    "        # log_f_x initialized to -Inf because log(0) = -Inf\n",
    "        log_b_x = np.zeros((self.n_states, n_x)) - np.Inf\n",
    "        x_emission_scores = np.array([hmm.scores['emission'][:, hmm.word_to_pos[w]] for w in x]).T\n",
    "        log_b_x[:,-1] = self.scores['final']\n",
    "\n",
    "        for n in range(n_x-2, -1, -1):\n",
    "            for s in range(self.n_states):\n",
    "                log_b_x[s,n] = logsum(log_b_x[:,n+1] + self.scores['transition'][:,s] + x_emission_scores[:,n+1])\n",
    "\n",
    "        log_likelihood = logsum(log_b_x[:,0] + self.scores['initial'] + x_emission_scores[:,0]) \n",
    "        return log_b_x, log_likelihood  # log(P(X=x))\n",
    "        \n",
    "    def predict_labels(self, x: list, decode=\"posterior\"):\n",
    "        \"\"\"\n",
    "        Retuns a sequence of states for each word in **x**.\n",
    "        The output depends on the **decode** method chosen.\n",
    "        \"\"\"\n",
    "        assert decode in self.decode, \"decode `{}` is not valid\".format(decode)\n",
    "        \n",
    "        if decode is 'posterior':\n",
    "            return self.posterior_decode(x)\n",
    "        \n",
    "        if decode is 'viterbi':\n",
    "            return self.viterbi_decode(x)\n",
    "\n",
    "    def compute_state_posteriors(self, x:list):\n",
    "        log_f_x, log_likelihood = self.log_forward_computations(x)\n",
    "        log_b_x, log_likelihood = self.log_backward_computations(x)\n",
    "        state_posteriors = np.zeros((self.n_states, len(x)))\n",
    "        \n",
    "        for pos in range(len(x)):\n",
    "            state_posteriors[:, pos] = log_f_x[:, pos] + log_b_x[:, pos] - log_likelihood\n",
    "        return state_posteriors\n",
    "\n",
    "    def posterior_decode(self, x: list, decode_states=True):\n",
    "        \n",
    "        state_posteriors = self.compute_state_posteriors(x)\n",
    "        y_hat = state_posteriors.argmax(axis=0)\n",
    "        \n",
    "        if decode_states:\n",
    "            y_hat = [hmm.pos_to_state[y] for y in y_hat]\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(word_to_pos, tag_to_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.7/site-packages/ipykernel_launcher.py:85: RuntimeWarning: divide by zero encountered in log\n",
      "/root/.local/lib/python3.7/site-packages/ipykernel_launcher.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "/root/.local/lib/python3.7/site-packages/ipykernel_launcher.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "/root/.local/lib/python3.7/site-packages/ipykernel_launcher.py:88: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "hmm.fit(X_train_txt, Y_train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07d9f35b3ae4f03834cb51695cbd3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.7/site-packages/ipykernel_launcher.py:161: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8624\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "err = 0\n",
    "\n",
    "mstks = []\n",
    "correct = []\n",
    "\n",
    "for i in tqdm(range(len(X_test_txt))):\n",
    "    pred = hmm.predict_labels(X_test_txt[i])\n",
    "    s = sum(v1!=v2 for v1,v2 in list(zip(pred, Y_test_txt[i])))\n",
    "    err+= s\n",
    "    tot+=len(Y_test_txt[i])\n",
    "    \n",
    "    toappend = mstks if s!=0 else correct\n",
    "    toappend.append(pd.DataFrame([X_test_txt[i], pred, Y_test_txt[i]]))\n",
    "    \n",
    "print(\"Accuracy: {:6.4f}\".format(1-err/tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>But</td>\n",
       "      <td>he</td>\n",
       "      <td>says</td>\n",
       "      <td>those</td>\n",
       "      <td>who</td>\n",
       "      <td>become</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>will</td>\n",
       "      <td>be</td>\n",
       "      <td>punished</td>\n",
       "      <td>without</td>\n",
       "      <td>mercy</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1     2      3    4       5        6     7   8         9       10  \\\n",
       "0  But  he  says  those  who  become  corrupt  will  be  punished  without   \n",
       "1    O   O     O      O    O       O        O     O   O         O        O   \n",
       "2    O   O     O      O    O       O        O     O   O         O        O   \n",
       "\n",
       "      11 12  \n",
       "0  mercy  .  \n",
       "1      O  O  \n",
       "2      O  O  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "      <td>has</td>\n",
       "      <td>lashed</td>\n",
       "      <td>out</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>report</td>\n",
       "      <td>critical</td>\n",
       "      <td>of</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>policies</td>\n",
       "      <td>on</td>\n",
       "      <td>religious</td>\n",
       "      <td>freedom</td>\n",
       "      <td>,</td>\n",
       "      <td>saying</td>\n",
       "      <td>such</td>\n",
       "      <td>criticism</td>\n",
       "      <td>could</td>\n",
       "      <td>harm</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "      <td>relations</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>O</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1       2    3   4  5      6       7         8   9       10  \\\n",
       "0  China  has  lashed  out  at  a   U.S.  report  critical  of  Chinese   \n",
       "1      O    O       O    O   O  O      O       O         O   O        O   \n",
       "2  B-gpe    O       O    O   O  O  B-gpe       O         O   O    B-gpe   \n",
       "\n",
       "         11  12         13       14 15      16    17         18     19    20  \\\n",
       "0  policies  on  religious  freedom  ,  saying  such  criticism  could  harm   \n",
       "1         O   O          O        O  O       O     O          O      O     O   \n",
       "2         O   O          O        O  O       O     O          O      O     O   \n",
       "\n",
       "      21 22     23         24 25  \n",
       "0   U.S.  -  China  relations  .  \n",
       "1      O  O      O          O  O  \n",
       "2  B-gpe  O  B-gpe          O  O  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mstks[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
