{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn import *\n",
    "import re\n",
    "\n",
    "from SimpleCountVectorizer import *\n",
    "from SimpleCountVectorizerAMC import *\n",
    "\n",
    "from TFIDFVectorizer import *\n",
    "from utils import *\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import pickle\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/quora_train_data.csv\")\n",
    "test_df = pd.read_csv('./data/quora_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 6), (80858, 6))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str'}\n"
     ]
    }
   ],
   "source": [
    "all_questions = cast_list_as_strings(list(train_df.loc[:, 'question1'])+list(train_df.loc[:, 'question2']))\n",
    "print(set(type(x).__name__ for x in all_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Tokenizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The tokenization function is the most important function of our CountVectorizer. It is in charge of deciding which tokens will represent a document (or phrase). As we can see, multiple functionalities have been added, which we will detail below:\n",
    "\n",
    "* **Stopwords**: deactivated by default, it removes the most common English words. \n",
    "    This functionality made us reduce the evaluation metrics in that specific problem but it is a good functionality to take into account in future projects.\n",
    "\n",
    "\n",
    "* **Numbers to words**: allows to solve problems like:\n",
    "    * Q1: How much is 2+2?\n",
    "    * Q2: What is the sum of two plus two?\n",
    "    \n",
    "    In this case the numbers are converted to their string representation thanks to a function implemented in the utils library.\n",
    "\n",
    "\n",
    "* **Stemmer and Lemmatizer**: Two great allies of any text model, they serve to standardize the words by converting them to their root word, remove the 's' from the plurals...\n",
    "\n",
    "\n",
    "* **N-grams**: To improve prediction and not use only tokens, we have introduced tuples of tokens. As in sklearn, we can specify the size of the N-grams with a function parameter.\n",
    "\n",
    "* **N-tokens**: We added an extra field to indicate the number of tokens of that document. This feature helps to improve accuracy.\n",
    "\n",
    "* **Duplicate question words**: In order to enhance the type of the question, we duplicate the keyword.\n",
    "\n",
    "* **Duplicate verbs**: Verbs are extremely important in deciphering the underlying meaning of a sentence. Therefore, we attributed more importance to them via duplication. \n",
    "\n",
    "* **Duplicate nouns**: Nouns are extremely important in deciphering the underlying meaning of a sentence. Therefore, we attributed more importance to them via duplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Fitting the improved SimpleCountVectorizer\n",
    "+ Thanks to pickle we load the data directly. For more details of the process check the notebook **F1_Building_the_model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "count_vect = pickle.load(open(\"models/CountVect.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Transforming the datasets into sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 9425768), (323432, 6), (80858, 9425768), (80858, 6))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2 = pickle.load(open(\"models/X_tr_q1q2.pkl\", 'rb'))\n",
    "X_te_q1q2 = pickle.load(open(\"models/X_te_q1q2.pkl\", 'rb'))\n",
    "\n",
    "## Checking shapes\n",
    "X_tr_q1q2.shape, train_df.shape, X_te_q1q2.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432,), (80858,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df[\"is_duplicate\"].values\n",
    "y_test = test_df['is_duplicate'].values\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Base model (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in training: 0.9990075193549185\n",
      "Accuracy in testing: 0.8131662915234115\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_linear_reg = pickle.load(open(\"models/model_lr_count.pkl\", 'rb'))\n",
    "result_train = loaded_linear_reg.score(X_tr_q1q2, y_train)\n",
    "result_test = loaded_linear_reg.score(X_te_q1q2, y_test)\n",
    "\n",
    "print(\"Accuracy in training:\", result_train)\n",
    "print(\"Accuracy in testing:\",result_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Improving results (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model_countvect = pickle.load(open(\"models/xgboost_model_countvect.pkl\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "results = xgb_model_countvect.evals_result()\n",
    "epochs = len(results['validation_0']['logloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "# plot log loss\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "ax.set_ylabel('Log Loss')\n",
    "ax.set_title('XGBoost Log Loss')\n",
    "\n",
    "# plot classification AUC\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "ax.legend()\n",
    "ax.set_ylabel('Classification AUC')\n",
    "ax.set_title('XGBoost Classification AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In our case, TFIDF has a lower performance than SimpleCountVectorizer (with the default parameters it already had it). We have managed to raise the score a little bit although we only use the CountVectorizer implemented at the beginning to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TFIDFVectorizer(count_vect.vocabulary, count_vect.word_to_ind, my_tokenizer_func)\n",
    "tfidf_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_tfidf_tr_q1q2 = get_features_from_df(train_df, tfidf_vectorizer)\n",
    "X_tfidf_te_q1q2  = get_features_from_df(test_df, tfidf_vectorizer)\n",
    "\n",
    "X_tfidf_tr_q1q2.shape, train_df.shape, test_df.shape, X_tfidf_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", verbose=1, max_iter=1000)\n",
    "logistic.fit(X_tfidf_tr_q1q2, y_train)\n",
    "\n",
    "logistic.score(X_tfidf_tr_q1q2, y_train), logistic.score(X_tfidf_te_q1q2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 10000 # With early stopping\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=N)\n",
    "xgb_model.fit(X_tfidf_tr_q1q2, y_train, \n",
    "              verbose=10, \n",
    "              eval_set=[(X_tfidf_tr_q1q2, y_train),(X_tfidf_te_q1q2, y_test)], \n",
    "              early_stopping_rounds =10,\n",
    "              eval_metric=['auc','logloss'],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "xgb_model.save_model('models/model_tfidf.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Cosas importantes:\n",
    "He subido el notebook final 1: F1_Building_the_model\n",
    "He separado algunas funciones en una libreria utils.py (int2num, cast2int)\n",
    "He creado tambien una libreria mistakes.py con las cuatro funciones de mistakes. Quien escriba el notebook, que lo tenga en cuenta de no ponerlas en el notebook, solo importar \n",
    "\n",
    "from utils import *\n",
    "from mistakes import *\n",
    "\n",
    "He generado dos modelos finales model_count.dat, model_tfidf.dat, que pueden ser cargados y comparados con los de sklearn. El primero llega a una AUC de 88% i el segundo a 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
