{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alejandro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alejandro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import scipy as sp\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.preprocessing\n",
    "import time\n",
    "stemmer =  SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_en(num):\n",
    "    d = { 0 : 'zero', 1 : 'one', 2 : 'two', 3 : 'three', 4 : 'four', 5 : 'five',\n",
    "          6 : 'six', 7 : 'seven', 8 : 'eight', 9 : 'nine', 10 : 'ten',\n",
    "          11 : 'eleven', 12 : 'twelve', 13 : 'thirteen', 14 : 'fourteen',\n",
    "          15 : 'fifteen', 16 : 'sixteen', 17 : 'seventeen', 18 : 'eighteen',\n",
    "          19 : 'nineteen', 20 : 'twenty',\n",
    "          30 : 'thirty', 40 : 'forty', 50 : 'fifty', 60 : 'sixty',\n",
    "          70 : 'seventy', 80 : 'eighty', 90 : 'ninety' }\n",
    "    k = 1000\n",
    "    m = k * 1000\n",
    "    b = m * 1000\n",
    "    t = b * 1000\n",
    "\n",
    "    assert(0 <= num)\n",
    "\n",
    "    if (num < 20):\n",
    "        return d[num]\n",
    "\n",
    "    if (num < 100):\n",
    "        if num % 10 == 0: return d[num]\n",
    "        else: return d[num // 10 * 10] + '-' + d[num % 10]\n",
    "\n",
    "    if (num < k):\n",
    "        if num % 100 == 0: return d[num // 100] + ' hundred'\n",
    "        else: return d[num // 100] + ' hundred and ' + int_to_en(num % 100)\n",
    "\n",
    "    if (num < m):\n",
    "        if num % k == 0: return int_to_en(num // k) + ' thousand'\n",
    "        else: return int_to_en(num // k) + ' thousand, ' + int_to_en(num % k)\n",
    "\n",
    "    if (num < b):\n",
    "        if (num % m) == 0: return int_to_en(num // m) + ' million'\n",
    "        else: return int_to_en(num // m) + ' million, ' + int_to_en(num % m)\n",
    "        \n",
    "    if (num < t):\n",
    "        if (num % b) == 0: return int_to_en(num // b) + ' billion'\n",
    "        else: return int_to_en(num // b) + ' billion, ' + int_to_en(num % b)\n",
    "\n",
    "    if (num % t == 0): return int_to_en(num // t) + ' trillion'\n",
    "    else: return int_to_en(num // t) + ' trillion, ' + int_to_en(num % t)\n",
    "\n",
    "    raise AssertionError('num is too large: %s' % str(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"quora_train_data.csv\")\n",
    "test_df = pd.read_csv(\"quora_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 6), (80858, 6))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All questions (data variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "all_q1_train = list(train_df[\"question1\"])\n",
    "all_q2_train = list(train_df[\"question2\"])\n",
    "all_questions_train = all_q1_train + all_q2_train\n",
    "len(all_questions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is_duplicate (target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323432, 80858)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df[\"is_duplicate\"].values\n",
    "y_test = test_df['is_duplicate'].values\n",
    "len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of questions into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str', 'float'}\n"
     ]
    }
   ],
   "source": [
    "print(set(type(x).__name__ for x in all_questions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    # return list(map(lambda x: str(x), all_questions)) # Slower\n",
    "    return [str(s) for s in mylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str'}\n"
     ]
    }
   ],
   "source": [
    "all_questions_train = cast_list_as_strings(all_questions_train)\n",
    "print(set(type(x).__name__ for x in all_questions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) CountVectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sklearn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "sk_vect = count_vect.fit(all_questions_train)\n",
    "sk_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_df(df, vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    q1 = vectorizer.transform(q1_casted)\n",
    "    q2 = vectorizer.transform(q2_casted)\n",
    "    \n",
    "    X_q1q2 = scipy.sparse.hstack((q1,q2))\n",
    "        \n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2 = get_features_from_df(train_df,count_vect)\n",
    "X_te_q1q2  = get_features_from_df(test_df, count_vect)\n",
    "\n",
    "X_tr_q1q2.shape, X_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver=\"liblinear\",verbose=1, max_iter=100)\n",
    "logistic.fit(X_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.811954290237206, 0.7536050854584581)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_0 = logistic.score(X_tr_q1q2, y_train)\n",
    "acc_test_0 = logistic.score(X_te_q1q2, y_test)\n",
    "acc_train_0 , acc_test_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCountVectorizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_word_counts=1,\n",
    "                 doc_cleaner_pattern=r\"[^a-zA-Z]\",\n",
    "                 token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 dtype=np.float32,\n",
    "                 doc_cleaner_func=None,\n",
    "                 tokenizer_func=None,\n",
    "                 word_transformer_func=None):\n",
    "        \n",
    "        self._retype = type(re.compile('hello, world'))\n",
    "\n",
    "        self.min_word_counts     = min_word_counts\n",
    "        self.doc_cleaner_pattern = doc_cleaner_pattern\n",
    "        self.token_pattern       = token_pattern\n",
    "        self.dtype               = dtype\n",
    "        \n",
    "        self.doc_cleaner_func      = doc_cleaner_func\n",
    "        self.tokenizer_func        = tokenizer_func\n",
    "        self.word_transformer_func = word_transformer_func\n",
    "\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_ind = {}\n",
    "\n",
    "\n",
    "    def build_doc_cleaner(self, lower=True):\n",
    "        \"\"\"\n",
    "        Returns a function that cleans undesirable substrings in a string.\n",
    "        It also lowers the input string if lower=True\n",
    "        \"\"\"\n",
    "        if self.doc_cleaner_func:\n",
    "            return self.doc_cleaner_func\n",
    "        else:\n",
    "            if isinstance(self.doc_cleaner_pattern, self._retype):\n",
    "                #clean_doc_pattern = self.doc_cleaner_pattern.sub(\" \", doc)\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "            else:\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "\n",
    "            if lower:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc).lower()\n",
    "            else:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc)\n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        \"\"\"Returns a function that splits a string into a sequence of tokens\"\"\"\n",
    "        if self.tokenizer_func:\n",
    "            return self.tokenizer_func\n",
    "        \n",
    "        else:\n",
    "            token_pattern = re.compile(self.token_pattern)\n",
    "            return lambda doc: token_pattern.findall(doc)\n",
    "\n",
    "    def build_word_transformer(self):\n",
    "        \"\"\"Returns a stemmer or lemmaitzer if object has any\"\"\"\n",
    "        \n",
    "        if self.word_transformer_func:\n",
    "            return self.word_transformer_func\n",
    "        else:\n",
    "            return lambda word: word\n",
    "        \n",
    "    def tokenize(self, doc):\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        doc     = doc_cleaner(doc)\n",
    "        words = doc_tokenizer(doc)\n",
    "            \n",
    "        return words\n",
    "        \n",
    "    def fit(self, X):\n",
    "\n",
    "        assert self.vocabulary == set(), \"self.vocabulary is not empty it has {} words\".format(len(self.vocabulary))\n",
    "        assert isinstance(X,list), \"X is expected to be a list of documents\"\n",
    "        \n",
    "        i = 0\n",
    "        word_to_ind = {}\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()\n",
    "        \n",
    "        for x in X:\n",
    "            words = doc_cleaner(x)\n",
    "            words = doc_tokenizer(words)\n",
    "            for word in words:\n",
    "                if word not in word_to_ind:\n",
    "                    word_to_ind[word] = i\n",
    "                    i += 1\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.n_features = len(word_to_ind)    \n",
    "                \n",
    "        self.vocabulary = set(word_to_ind.keys())\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, memory_efficient=False):\n",
    "        \n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()      \n",
    "        \n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "                \n",
    "        if memory_efficient:\n",
    "            for m, x in enumerate(X):  \n",
    "                words = doc_cleaner(x)\n",
    "                words = doc_tokenizer(words)\n",
    "                for word in words: \n",
    "                    index = self.word_to_ind[word]\n",
    "                    \n",
    "                    col_indices.append(index)\n",
    "                    row_indices.append(m)\n",
    "                    sp_data.append(1)\n",
    "                    \n",
    "            encoded_X = sp.csr_matrix((sp_data, (row_indices, col_indices)), shape = (len(X) ,self.n_features)) \n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            encoded_X = np.zeros((len(X), len(self.word_to_ind)))\n",
    "            for m, x in enumerate(X):  \n",
    "                words = doc_cleaner(x)\n",
    "                words = doc_tokenizer(words)\n",
    "                for word in words: \n",
    "                    index = self.word_to_ind[word]\n",
    "                    encoded_X[m, index] += 1\n",
    "        \n",
    "        return encoded_X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        encoded_X = self.transform(X)\n",
    "        return encoded_X\n",
    "    \n",
    "    def _words_in_vocab(self, X):\n",
    "        \n",
    "        if isinstance(X, str):\n",
    "            return [w for w in self.tokenize(X) if w in self.vocabulary]\n",
    "        \n",
    "        X_words_in_vocab = []\n",
    "        for sentence in X:\n",
    "            X_words_in_vocab.append(self.tokenize(sentence))\n",
    "            \n",
    "        return X_words_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_conv(s):\n",
    "    try:\n",
    "        return int_to_en(int(s)).replace(\",\",\"\").replace(\" \",\"_\")\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "\n",
    "def my_doc_cleaner(doc,\n",
    "                  pat=r\"[^a-zA-Z0-9]\"):\n",
    "    # Allow alphanumeric characters\n",
    "    doc_cleaner_pattern=pat\n",
    "    clean_doc_pattern = re.compile(doc_cleaner_pattern)\n",
    "    doc_clean = clean_doc_pattern.sub(\" \", doc)\n",
    "    return doc.lower()\n",
    "\n",
    "\n",
    "# stpw = set(stopwords.words(\"english\"))\n",
    "stpw = []\n",
    "\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer_func(doc, \n",
    "                      ngrams=(1,3), \n",
    "                      numbers_to_words=True,\n",
    "                      stop_words=stpw,\n",
    "                      pat=r\"(?u)\\b\\w\\S*\\w*\\b\",\n",
    "                      lem=True,\n",
    "                      stem=True):\n",
    "    \n",
    "    # Split using a patterm\n",
    "#     pat=r\"(?u)\\b\\w\\w+\\b\"\n",
    "#     pat=r\"(?u)\\b\\w\\S*\\w*\\b\"\n",
    "    token_pattern = re.compile(pat)\n",
    "    lst = token_pattern.findall(doc)\n",
    "    \n",
    "    # Transform numbers into words\n",
    "    if numbers_to_words:\n",
    "        lst = list(map(lambda x: num_conv(x), lst))\n",
    "        \n",
    "    # Drop stopwords \n",
    "    lst = list(filter(lambda x : x not in stop_words, lst))\n",
    "    \n",
    "    #Stemmer\n",
    "    if stem:\n",
    "        lst = list(map(lambda x: stemmer.stem(x), lst))\n",
    "    \n",
    "    #Lemmatizer \n",
    "    if lem:\n",
    "        lst = list(map(lambda x: lemmatizer.lemmatize(x), lst))\n",
    "        \n",
    "    if ngrams==(1,1):\n",
    "        return lst\n",
    "    \n",
    "    # Generate ngrams\n",
    "    lstRet = []\n",
    "    for a in range(ngrams[0], ngrams[1]+1):\n",
    "        if a!=1:\n",
    "            lstRet+=list(zip(*[lst[i:] for i in range(a)]))\n",
    "    return lstRet if ngrams[0]!=1 else lst+lstRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCountVectorizer(doc_cleaner_func=<function my_doc_cleaner at 0x0000020C8F02E400>,\n",
       "                      doc_cleaner_pattern='[^a-zA-Z]',\n",
       "                      dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "                      token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                      tokenizer_func=<function my_tokenizer_func at 0x0000020C8F02E488>,\n",
       "                      word_transformer_func=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_simple = SimpleCountVectorizer(\n",
    "    doc_cleaner_func=my_doc_cleaner,\n",
    "    tokenizer_func=my_tokenizer_func\n",
    ")\n",
    "simple_vect = count_vect_simple.fit(all_questions_train)\n",
    "simple_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tr_q1q2_simple = get_features_from_df(train_df, count_vect_simple)\n",
    "#X_te_q1q2_simple  = get_features_from_df(test_df, count_vect_simple)\n",
    "\n",
    "#X_tr_q1q2_simple.shape, X_te_q1q2_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic.fit(X_tr_q1q2_simple, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_train_1 = logistic.score(X_tr_q1q2_simple, y_train)\n",
    "#acc_test_1 = logistic.score(X_te_q1q2_simple, y_test)\n",
    "#acc_train_1 , acc_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_1 = 0.9963670879813995\n",
    "acc_test_1 = 0.8085285314996661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) TfidfVectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sklearn TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "sk_tfidf = tfidf_vectorizer.fit(all_questions_train)\n",
    "sk_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_tr_q1q2 = get_features_from_df(train_df, tfidf_vectorizer)\n",
    "X_tfidf_te_q1q2  = get_features_from_df(test_df, tfidf_vectorizer)\n",
    "\n",
    "X_tfidf_tr_q1q2.shape, X_tfidf_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_tfidf_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7935763931831111, 0.7578347226001138)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_2 = logistic.score(X_tfidf_tr_q1q2, y_train)\n",
    "acc_test_2 = logistic.score(X_tfidf_te_q1q2, y_test)\n",
    "acc_train_2 , acc_test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTFIDFVectorizer():\n",
    "    \n",
    "    def __init__(self, vocabulary, word_to_ind, tokenize, normalize_tf=True, normalize_tfidf=True):\n",
    "        self.tokenize = tokenize\n",
    "        self.vocabulary = vocabulary\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.n_features = len(word_to_ind)\n",
    "        self.normalize_tf = normalize_tf\n",
    "        self.normalize_tfidf = normalize_tfidf\n",
    "        self.X_w = None\n",
    "        self.idf = None\n",
    "        self.n_documents = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit TFID vectorizer to a certain corpus of documents X\n",
    "        \"\"\"\n",
    "        assert isinstance(X,list), \"You should pass a list\"\n",
    "        \n",
    "        t1 = time.time()\n",
    "        self.__build_vocabulary(X)\n",
    "        self.n_documents = len(X)\n",
    "        self.__compute_idf()\n",
    "        print('TFIDF fit finished in',str(round(time.time()-t1, 2)),'seconds')\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform a corpus X to its TFID vectorization\n",
    "        \"\"\"\n",
    "        assert self.X_w is not None and self.idf is not None and self.n_documents is not None,'Fit must be performed first'\n",
    "        assert isinstance(X,list), \"You should pass a list\"\n",
    "        \n",
    "        t1 = time.time()\n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "        \n",
    "        encoded_X = None # Create an encoded_X\n",
    "        for m, doc in enumerate(X):\n",
    "#             print(m)\n",
    "            words = self.tokenize(doc)\n",
    "            for w in words:\n",
    "                if w in self.word_to_ind:\n",
    "                    index = self.word_to_ind[w]\n",
    "                    col_indices.append(index)\n",
    "                    row_indices.append(m)\n",
    "                    sp_data.append(1)\n",
    "#             print(doc, normalize_tf)\n",
    "#             print(len(doc))\n",
    "#             tf = self.__term_frequency(doc, normalize_tf)\n",
    "#             tfidf = tf.multiply(self.idf)\n",
    "#             if normalize_tfidf: tfidf = tfidf/sp.sparse.linalg.norm(tfidf)\n",
    "#             encoded_X = sp.vstack((encoded_X, tf)) if encoded_X is not None else tfidf\n",
    "        encoded_X = sp.sparse.csr_matrix((sp_data, (row_indices, col_indices)), shape=(len(X), self.n_features))\n",
    "        if self.normalize_tf: encoded_X = sklearn.preprocessing.normalize(encoded_X, axis=1)\n",
    "        \n",
    "        encoded_X = encoded_X.multiply(self.idf)\n",
    "        if self.normalize_tfidf: encoded_X = sklearn.preprocessing.normalize(encoded_X, axis=1)\n",
    "        \n",
    "        print('TFIDF transform finished in',str(round(time.time()-t1, 2)),'seconds')\n",
    "        return encoded_X\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def __build_vocabulary(self, corpus):\n",
    "        \"\"\"\n",
    "        This function builds X_w, a dict containing for each key, how\n",
    "        many documents having that key are in our corpus.\n",
    "        \"\"\"\n",
    "        X_w = {}\n",
    "\n",
    "        for document in corpus:\n",
    "            words = self.tokenize(document)\n",
    "            for word in words:\n",
    "                if word not in X_w: X_w[word] = 1\n",
    "                else: X_w[word] += 1\n",
    "\n",
    "        self.X_w = X_w\n",
    "        \n",
    "    def __compute_idf(self):\n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "\n",
    "        for w in self.X_w:\n",
    "            docs_present = self.X_w[w]\n",
    "            index = self.word_to_ind[w]\n",
    "            col_indices.append(index)\n",
    "            row_indices.append(0)\n",
    "            sp_data.append( np.log(self.n_documents / (1 + docs_present)) )\n",
    "\n",
    "        self.idf = sp.sparse.csr_matrix((sp_data, (row_indices, col_indices)), shape=(1, self.n_features))\n",
    "\n",
    "    def __term_frequency(self, document, normalize=True):\n",
    "        \n",
    "        words = self.tokenize(document)\n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "\n",
    "        for w in words:\n",
    "            if w in self.word_to_ind:\n",
    "                index = self.word_to_ind[w]\n",
    "                col_indices.append(index)\n",
    "                row_indices.append(0)\n",
    "                sp_data.append(1)\n",
    "        \n",
    "        tf = sp.sparse.csr_matrix((sp_data, (row_indices, col_indices)), shape=(1, self.n_features))\n",
    "        \n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF fit finished in 786.86 seconds\n"
     ]
    }
   ],
   "source": [
    "tfidf_simple = SimpleTFIDFVectorizer(count_vect_simple.vocabulary, count_vect_simple.word_to_ind, count_vect_simple.tokenize)\n",
    "simple_tfidf = tfidf_simple.fit(all_questions_train)\n",
    "simple_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF transform finished in 442.36 seconds\n",
      "TFIDF transform finished in 486.21 seconds\n",
      "TFIDF transform finished in 103.72 seconds\n",
      "TFIDF transform finished in 107.24 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((323432, 6166022), (80858, 6166022))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_simple_tr_q1q2 = get_features_from_df(train_df, tfidf_simple)\n",
    "X_tfidf_simple_te_q1q2  = get_features_from_df(test_df, tfidf_simple)\n",
    "\n",
    "X_tfidf_simple_tr_q1q2.shape, X_tfidf_simple_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_tfidf_simple_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9006870068515175, 0.7897919810037349)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_3 = logistic.score(X_tfidf_simple_tr_q1q2, y_train)\n",
    "acc_test_3 = logistic.score(X_tfidf_simple_te_q1q2, y_test)\n",
    "acc_train_3 , acc_test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results[\"Sklearn Countvectorizer\"]   = [acc_train_0, acc_test_0]\n",
    "df_results[\"Simple Countvectorizer\"]  = [acc_train_1, acc_test_1]\n",
    "df_results[\"Sklearn TfidfVectorizer\"] = [acc_train_2, acc_test_2]\n",
    "df_results[\"Simple TfidfVectorizer\"] = [acc_train_3, acc_test_3]\n",
    "df_results.index=[\"train\",\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sklearn Countvectorizer</th>\n",
       "      <th>Simple Countvectorizer</th>\n",
       "      <th>Sklearn TfidfVectorizer</th>\n",
       "      <th>Simple TfidfVectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.811954</td>\n",
       "      <td>0.996367</td>\n",
       "      <td>0.793576</td>\n",
       "      <td>0.900687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.753605</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>0.757835</td>\n",
       "      <td>0.789792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sklearn Countvectorizer  Simple Countvectorizer  \\\n",
       "train                 0.811954                0.996367   \n",
       "test                  0.753605                0.808529   \n",
       "\n",
       "       Sklearn TfidfVectorizer  Simple TfidfVectorizer  \n",
       "train                 0.793576                0.900687  \n",
       "test                  0.757835                0.789792  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20cf1dcd6a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.75,0.81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2.shape, X_te_q1q2.shape, #X_tr_q1q2_simple.shape, X_te_q1q2_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550), (323432, 6166022), (80858, 6166022))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_tr_q1q2.shape, X_tfidf_te_q1q2.shape, X_tfidf_simple_tr_q1q2.shape, X_tfidf_simple_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
