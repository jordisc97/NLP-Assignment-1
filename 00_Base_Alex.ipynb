{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.preprocessing\n",
    "import time\n",
    "stemmer =  SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"quora_train_data.csv\")\n",
    "test_df = pd.read_csv(\"quora_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 6), (80858, 6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All questions (data variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646864"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "all_q1_train = list(train_df[\"question1\"])\n",
    "all_q2_train = list(train_df[\"question2\"])\n",
    "all_questions_train = all_q1_train + all_q2_train\n",
    "len(all_questions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is_duplicate (target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323432, 80858)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df[\"is_duplicate\"].values\n",
    "y_test = test_df['is_duplicate'].values\n",
    "len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of questions into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str', 'float'}\n"
     ]
    }
   ],
   "source": [
    "print(set(type(x).__name__ for x in all_questions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    # return list(map(lambda x: str(x), all_questions)) # Slower\n",
    "    return [str(s) for s in mylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str'}\n"
     ]
    }
   ],
   "source": [
    "all_questions_train = cast_list_as_strings(all_questions_train)\n",
    "print(set(type(x).__name__ for x in all_questions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) CountVectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sklearn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "sk_vect = count_vect.fit(all_questions_train)\n",
    "sk_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_df(df, vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    q1 = vectorizer.transform(q1_casted)\n",
    "    q2 = vectorizer.transform(q2_casted)\n",
    "    \n",
    "    X_q1q2 = scipy.sparse.hstack((q1,q2))\n",
    "        \n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2 = get_features_from_df(train_df,count_vect)\n",
    "X_te_q1q2  = get_features_from_df(test_df, count_vect)\n",
    "\n",
    "X_tr_q1q2.shape, X_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(X_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.811954290237206, 0.7536050854584581)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_0 = logistic.score(X_tr_q1q2, y_train)\n",
    "acc_test_0 = logistic.score(X_te_q1q2, y_test)\n",
    "acc_train_0 , acc_test_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCountVectorizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_word_counts=1,\n",
    "                 doc_cleaner_pattern=r\"[^a-zA-Z]\",\n",
    "                 token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 dtype=np.float32,\n",
    "                 doc_cleaner_func=None,\n",
    "                 tokenizer_func=None,\n",
    "                 word_transformer_func=None):\n",
    "        \n",
    "        self._retype = type(re.compile('hello, world'))\n",
    "\n",
    "        self.min_word_counts     = min_word_counts\n",
    "        self.doc_cleaner_pattern = doc_cleaner_pattern\n",
    "        self.token_pattern       = token_pattern\n",
    "        self.dtype               = dtype\n",
    "        \n",
    "        self.doc_cleaner_func      = doc_cleaner_func\n",
    "        self.tokenizer_func        = tokenizer_func\n",
    "        self.word_transformer_func = word_transformer_func\n",
    "\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_ind = {}\n",
    "\n",
    "\n",
    "    def build_doc_cleaner(self, lower=True):\n",
    "        \"\"\"\n",
    "        Returns a function that cleans undesirable substrings in a string.\n",
    "        It also lowers the input string if lower=True\n",
    "        \"\"\"\n",
    "        if self.doc_cleaner_func:\n",
    "            return self.doc_cleaner_func\n",
    "        else:\n",
    "            if isinstance(self.doc_cleaner_pattern, self._retype):\n",
    "                #clean_doc_pattern = self.doc_cleaner_pattern.sub(\" \", doc)\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "            else:\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "\n",
    "            if lower:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc).lower()\n",
    "            else:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc)\n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        \"\"\"Returns a function that splits a string into a sequence of tokens\"\"\"\n",
    "        if self.tokenizer_func:\n",
    "            return self.tokenizer_func\n",
    "        \n",
    "        else:\n",
    "            token_pattern = re.compile(self.token_pattern)\n",
    "            return lambda doc: token_pattern.findall(doc)\n",
    "\n",
    "    def build_word_transformer(self):\n",
    "        \"\"\"Returns a stemmer or lemmaitzer if object has any\"\"\"\n",
    "        \n",
    "        if self.word_transformer_func:\n",
    "            return self.word_transformer_func\n",
    "        else:\n",
    "            return lambda word: word\n",
    "        \n",
    "    def tokenize(self, doc):\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        doc     = doc_cleaner(doc)\n",
    "        words = doc_tokenizer(doc)\n",
    "            \n",
    "        return words\n",
    "        \n",
    "    def fit(self, X):\n",
    "\n",
    "        assert self.vocabulary == set(), \"self.vocabulary is not empty it has {} words\".format(len(self.vocabulary))\n",
    "        assert isinstance(X,list), \"X is expected to be a list of documents\"\n",
    "        \n",
    "        i = 0\n",
    "        word_to_ind = {}\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()\n",
    "        \n",
    "        for x in X:\n",
    "            words = doc_cleaner(x)\n",
    "            words = doc_tokenizer(words)\n",
    "            for word in words:\n",
    "                if word not in word_to_ind:\n",
    "                    word_to_ind[word] = i\n",
    "                    i += 1\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.n_features = len(word_to_ind)    \n",
    "                \n",
    "        self.vocabulary = set(word_to_ind.keys())\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, memory_efficient=False):\n",
    "        \n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()      \n",
    "        \n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "                \n",
    "        if memory_efficient:\n",
    "            for m, x in enumerate(X):  \n",
    "                words = doc_cleaner(x)\n",
    "                words = doc_tokenizer(words)\n",
    "                for word in words: \n",
    "                    index = self.word_to_ind[word]\n",
    "                    \n",
    "                    col_indices.append(index)\n",
    "                    row_indices.append(m)\n",
    "                    sp_data.append(1)\n",
    "                    \n",
    "            encoded_X = sp.csr_matrix((sp_data, (row_indices, col_indices)), shape = (len(X) ,self.n_features)) \n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            encoded_X = np.zeros((len(X), len(self.word_to_ind)))\n",
    "            for m, x in enumerate(X):  \n",
    "                words = doc_cleaner(x)\n",
    "                words = doc_tokenizer(words)\n",
    "                for word in words: \n",
    "                    index = self.word_to_ind[word]\n",
    "                    encoded_X[m, index] += 1\n",
    "        \n",
    "        return encoded_X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        encoded_X = self.transform(X)\n",
    "        return encoded_X\n",
    "    \n",
    "    def _words_in_vocab(self, X):\n",
    "        \n",
    "        if isinstance(X, str):\n",
    "            return [w for w in self.tokenize(X) if w in self.vocabulary]\n",
    "        \n",
    "        X_words_in_vocab = []\n",
    "        for sentence in X:\n",
    "            X_words_in_vocab.append(self.tokenize(sentence))\n",
    "            \n",
    "        return X_words_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCountVectorizer(doc_cleaner_func=None, doc_cleaner_pattern='[^a-zA-Z]',\n",
       "                      dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "                      token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer_func=None,\n",
       "                      word_transformer_func=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_simple = SimpleCountVectorizer()\n",
    "simple_vect = count_vect_simple.fit(all_questions_train)\n",
    "simple_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tr_q1q2_simple = get_features_from_df(train_df, count_vect_simple)\n",
    "#X_te_q1q2_simple  = get_features_from_df(test_df, count_vect_simple)\n",
    "\n",
    "#X_tr_q1q2_simple.shape, X_te_q1q2_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic.fit(X_tr_q1q2_simple, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_train_1 = logistic.score(X_tr_q1q2_simple, y_train)\n",
    "#acc_test_1 = logistic.score(X_te_q1q2_simple, y_test)\n",
    "#acc_train_1 , acc_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_1 = 0.8086862153404735\n",
    "acc_test_1 = 0.7522323084914294"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) TfidfVectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sklearn TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "sk_tfidf = tfidf_vectorizer.fit(all_questions_train)\n",
    "sk_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_tr_q1q2 = get_features_from_df(train_df, tfidf_vectorizer)\n",
    "X_tfidf_te_q1q2  = get_features_from_df(test_df, tfidf_vectorizer)\n",
    "\n",
    "X_tfidf_tr_q1q2.shape, X_tfidf_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_tfidf_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7935763931831111, 0.7578347226001138)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_2 = logistic.score(X_tfidf_tr_q1q2, y_train)\n",
    "acc_test_2 = logistic.score(X_tfidf_te_q1q2, y_test)\n",
    "acc_train_2 , acc_test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTFIDFVectorizer():\n",
    "    \n",
    "    def __init__(self, vocabulary, word_to_ind, tokenize, normalize_tf=True, normalize_tfidf=True):\n",
    "        self.tokenize = tokenize\n",
    "        self.vocabulary = vocabulary\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.n_features = len(word_to_ind)\n",
    "        self.normalize_tf = normalize_tf\n",
    "        self.normalize_tfidf = normalize_tfidf\n",
    "        self.X_w = None\n",
    "        self.idf = None\n",
    "        self.n_documents = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit TFID vectorizer to a certain corpus of documents X\n",
    "        \"\"\"\n",
    "        assert isinstance(X,list), \"You should pass a list\"\n",
    "        \n",
    "        t1 = time.time()\n",
    "        self.__build_vocabulary(X)\n",
    "        self.n_documents = len(X)\n",
    "        self.__compute_idf()\n",
    "        print('TFIDF fit finished in',str(round(time.time()-t1, 2)),'seconds')\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform a corpus X to its TFID vectorization\n",
    "        \"\"\"\n",
    "        assert self.X_w is not None and self.idf is not None and self.n_documents is not None,'Fit must be performed first'\n",
    "        assert isinstance(X,list), \"You should pass a list\"\n",
    "        \n",
    "        t1 = time.time()\n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "        \n",
    "        encoded_X = None # Create an encoded_X\n",
    "        for m, doc in enumerate(X):\n",
    "#             print(m)\n",
    "            words = self.tokenize(doc)\n",
    "            for w in words:\n",
    "                if w in self.word_to_ind:\n",
    "                    index = self.word_to_ind[w]\n",
    "                    col_indices.append(index)\n",
    "                    row_indices.append(m)\n",
    "                    sp_data.append(1)\n",
    "#             print(doc, normalize_tf)\n",
    "#             print(len(doc))\n",
    "#             tf = self.__term_frequency(doc, normalize_tf)\n",
    "#             tfidf = tf.multiply(self.idf)\n",
    "#             if normalize_tfidf: tfidf = tfidf/sp.sparse.linalg.norm(tfidf)\n",
    "#             encoded_X = sp.vstack((encoded_X, tf)) if encoded_X is not None else tfidf\n",
    "        encoded_X = sp.sparse.csr_matrix((sp_data, (row_indices, col_indices)), shape=(len(X), self.n_features))\n",
    "        if self.normalize_tf: encoded_X = sklearn.preprocessing.normalize(encoded_X, axis=1)\n",
    "        \n",
    "        encoded_X = encoded_X.multiply(self.idf)\n",
    "        if self.normalize_tfidf: encoded_X = sklearn.preprocessing.normalize(encoded_X, axis=1)\n",
    "        \n",
    "        print('TFIDF transform finished in',str(round(time.time()-t1, 2)),'seconds')\n",
    "        return encoded_X\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def __build_vocabulary(self, corpus):\n",
    "        \"\"\"\n",
    "        This function builds X_w, a dict containing for each key, how\n",
    "        many documents having that key are in our corpus.\n",
    "        \"\"\"\n",
    "        X_w = {}\n",
    "\n",
    "        for document in corpus:\n",
    "            words = self.tokenize(document)\n",
    "            for word in words:\n",
    "                if word not in X_w: X_w[word] = 1\n",
    "                else: X_w[word] += 1\n",
    "\n",
    "        self.X_w = X_w\n",
    "        \n",
    "    def __compute_idf(self):\n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "\n",
    "        for w in self.X_w:\n",
    "            docs_present = self.X_w[w]\n",
    "            index = self.word_to_ind[w]\n",
    "            col_indices.append(index)\n",
    "            row_indices.append(0)\n",
    "            sp_data.append( np.log(self.n_documents / (1 + docs_present)) )\n",
    "\n",
    "        self.idf = sp.sparse.csr_matrix((sp_data, (row_indices, col_indices)), shape=(1, self.n_features))\n",
    "\n",
    "    def __term_frequency(self, document, normalize=True):\n",
    "        \n",
    "        words = self.tokenize(document)\n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "\n",
    "        for w in words:\n",
    "            if w in self.word_to_ind:\n",
    "                index = self.word_to_ind[w]\n",
    "                col_indices.append(index)\n",
    "                row_indices.append(0)\n",
    "                sp_data.append(1)\n",
    "        \n",
    "        tf = sp.sparse.csr_matrix((sp_data, (row_indices, col_indices)), shape=(1, self.n_features))\n",
    "        \n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF fit finished in 33.7 seconds\n"
     ]
    }
   ],
   "source": [
    "tfidf_simple = SimpleTFIDFVectorizer(count_vect_simple.vocabulary, count_vect_simple.word_to_ind, count_vect_simple.tokenize)\n",
    "simple_tfidf = tfidf_simple.fit(all_questions_train)\n",
    "simple_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF transform finished in 23.68 seconds\n",
      "TFIDF transform finished in 24.43 seconds\n",
      "TFIDF transform finished in 5.94 seconds\n",
      "TFIDF transform finished in 5.99 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((323432, 144312), (80858, 144312))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_simple_tr_q1q2 = get_features_from_df(train_df, tfidf_simple)\n",
    "X_tfidf_simple_te_q1q2  = get_features_from_df(test_df, tfidf_simple)\n",
    "\n",
    "X_tfidf_simple_tr_q1q2.shape, X_tfidf_simple_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_tfidf_simple_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7930878824606099, 0.7559301491503624)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_3 = logistic.score(X_tfidf_simple_tr_q1q2, y_train)\n",
    "acc_test_3 = logistic.score(X_tfidf_simple_te_q1q2, y_test)\n",
    "acc_train_3 , acc_test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results[\"Sklearn Countvectorizer\"]   = [acc_train_0, acc_test_0]\n",
    "df_results[\"Simple Countvectorizer\"]  = [acc_train_1, acc_test_1]\n",
    "df_results[\"Sklearn TfidfVectorizer\"] = [acc_train_2, acc_test_2]\n",
    "df_results[\"Simple TfidfVectorizer\"] = [acc_train_3, acc_test_3]\n",
    "df_results.index=[\"train\",\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sklearn Countvectorizer</th>\n",
       "      <th>Simple Countvectorizer</th>\n",
       "      <th>Sklearn TfidfVectorizer</th>\n",
       "      <th>Simple TfidfVectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.811954</td>\n",
       "      <td>0.808686</td>\n",
       "      <td>0.793576</td>\n",
       "      <td>0.793088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.753605</td>\n",
       "      <td>0.752232</td>\n",
       "      <td>0.757835</td>\n",
       "      <td>0.755930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sklearn Countvectorizer  Simple Countvectorizer  \\\n",
       "train                 0.811954                0.808686   \n",
       "test                  0.753605                0.752232   \n",
       "\n",
       "       Sklearn TfidfVectorizer  Simple TfidfVectorizer  \n",
       "train                 0.793576                0.793088  \n",
       "test                  0.757835                0.755930  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x165069d0470>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.75,0.76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2.shape, X_te_q1q2.shape, #X_tr_q1q2_simple.shape, X_te_q1q2_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (80858, 156550), (323432, 144312), (80858, 144312))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_tr_q1q2.shape, X_tfidf_te_q1q2.shape, X_tfidf_simple_tr_q1q2.shape, X_tfidf_simple_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
