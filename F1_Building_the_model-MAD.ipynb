{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn import *\n",
    "import re\n",
    "\n",
    "from SimpleCountVectorizer import *\n",
    "from SimpleCountVectorizerAMC import *\n",
    "\n",
    "from TFIDFVectorizer import *\n",
    "from utils import *\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/quora_train_data.csv\")\n",
    "test_df = pd.read_csv('./data/quora_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 6), (80858, 6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str'}\n"
     ]
    }
   ],
   "source": [
    "all_questions = cast_list_as_strings(list(train_df.loc[:, 'question1'])+list(train_df.loc[:, 'question2']))\n",
    "print(set(type(x).__name__ for x in all_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Document cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This function manages the cleaning of a document. After trying several things, we decided to simply filter by alphanumeric characters and replace the upper case with lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def my_doc_cleaner(doc,\n",
    "                  pat=r\"[^a-zA-Z0-9]\"):\n",
    "    \"\"\"\n",
    "    Document cleaner. We allow alphanumeric characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Allow alphanumeric characters\n",
    "    doc_cleaner_pattern=pat\n",
    "    clean_doc_pattern = re.compile(doc_cleaner_pattern)\n",
    "    doc_clean = clean_doc_pattern.sub(\" \", doc)\n",
    "    return doc.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Tokenizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The tokenization function is the most important function of our CountVectorizer. It is in charge of deciding which tokens will represent a document (or phrase). As we can see, multiple functionalities have been added, which we will detail below:\n",
    "\n",
    "* **Stopwords**: deactivated by default, it removes the most common English words. \n",
    "    This functionality made us reduce the evaluation metrics in that specific problem but it is a good functionality to take into account in future projects.\n",
    "\n",
    "\n",
    "* **Numbers to words**: allows to solve problems like:\n",
    "    * Q1: How much is 2+2?\n",
    "    * Q2: What is the sum of two plus two?\n",
    "    \n",
    "    In this case the numbers are converted to their string representation thanks to a function implemented in the utils library.\n",
    "\n",
    "\n",
    "* **Stemmer and Lemmatizer**: Two great allies of any text model, they serve to standardize the words by converting them to their root word, remove the 's' from the plurals...\n",
    "\n",
    "\n",
    "* **N-grams**: To improve prediction and not use only tokens, we have introduced tuples of tokens. As in sklearn, we can specify the size of the N-grams with a function parameter.\n",
    "\n",
    "* **N-tokens**: We added an extra field to indicate the number of tokens of that document. This feature helps to improve accuracy.\n",
    "\n",
    "* **Duplicate question words**: In order to enhance the type of the question, we duplicate the keyword.\n",
    "\n",
    "* **Duplicate verbs**: Verbs are extremely important in deciphering the underlying meaning of a sentence. Therefore, we attributed more importance to them via duplication. \n",
    "\n",
    "* **Duplicate nouns**: Nouns are extremely important in deciphering the underlying meaning of a sentence. Therefore, we attributed more importance to them via duplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# stpw = set(stopwords.words(\"english\"))\n",
    "stpw = []\n",
    "question_words = ['who','what','when','where','why','how','which']\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def my_tokenizer_func(doc, \n",
    "                      ngrams=(1,3), \n",
    "                      numbers_to_words=True,\n",
    "                      stop_words=stpw,\n",
    "                      duplicate_question_words=question_words,\n",
    "                      duplicate_verbs=False,\n",
    "                      duplicate_nouns=True,\n",
    "                      pat=r\"(?u)\\b\\w\\S*\\w*\\b\",\n",
    "                      lem=True,\n",
    "                      stem=True,\n",
    "                      add_num_tokens=True):\n",
    "    \n",
    "    # Split using a pattern\n",
    "    # Notice that the pattern has been changed and now it accepts a wider\n",
    "    # range of words. \n",
    "    \n",
    "    # Example: V2.3.4 (A version of a program) will be transformed into 'v2.3.4'\n",
    "    token_pattern = re.compile(pat)\n",
    "    lst = token_pattern.findall(doc)\n",
    "    \n",
    "    # Transform numbers into words\n",
    "    if numbers_to_words:\n",
    "        lst = list(map(lambda x: num_conv(x), lst))\n",
    "        \n",
    "    # Drop stopwords \n",
    "    lst = list(filter(lambda x : x not in stop_words, lst))\n",
    "    \n",
    "    # Duplicate key_words\n",
    "    if len(duplicate_question_words)>0:\n",
    "        lst += [value for value in lst if value.lower() in duplicate_question_words]\n",
    "    \n",
    "     # Duplicate verbs\n",
    "    if duplicate_verbs:\n",
    "        lst += [x[0] for x in nltk.pos_tag(lst,tagset='universal') if x[1] == 'VERB' and len(x[0]) > 1]\n",
    "        \n",
    "    # Duplicate nouns\n",
    "    if duplicate_nouns:\n",
    "        lst += [x[0] for x in nltk.pos_tag(lst,tagset='universal') if x[1] == 'NOUN' and len(x[0]) > 1]\n",
    "    \n",
    "    #Stemmer\n",
    "    if stem:\n",
    "        lst = list(map(lambda x: stemmer.stem(x), lst))\n",
    "    \n",
    "    #Lemmatizer \n",
    "    if lem:\n",
    "        lst = list(map(lambda x: lemmatizer.lemmatize(x), lst))\n",
    "    \n",
    "    # N-tokens\n",
    "    ntoks = []\n",
    "    if add_num_tokens:\n",
    "        ntoks = [num_conv(str(len(lst))) + '_tokens']\n",
    "    \n",
    "    if ngrams==(1,1):\n",
    "        return lst+ntoks\n",
    "    \n",
    "    # Generate ngrams\n",
    "    lstRet = []\n",
    "    for a in range(ngrams[0], ngrams[1]+1):\n",
    "        if a!=1:\n",
    "            lstRet+=list(zip(*[lst[i:] for i in range(a)]))\n",
    "    return lstRet+ntoks if ngrams[0]!=1 else lst+lstRet+ntoks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Fitting the improved SimpleCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78fb06f8ffd4b0788dc8bfbbab8407a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=646864.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCountVectorizerAMC(doc_cleaner_func=<function my_doc_cleaner at 0x000001FA8A018EE8>,\n",
       "                         doc_cleaner_pattern='[^a-zA-Z]',\n",
       "                         dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "                         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                         tokenizer_func=<function my_tokenizer_func at 0x000001FA87DD19D8>,\n",
       "                         word_transformer_func=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = SimpleCountVectorizerAMC(\n",
    "    doc_cleaner_func=my_doc_cleaner,\n",
    "    tokenizer_func=my_tokenizer_func\n",
    ")\n",
    "count_vect.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Transforming the datasets into sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_features_from_df(df, vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    q1 = vectorizer.transform(q1_casted)\n",
    "    q2 = vectorizer.transform(q2_casted)\n",
    "    \n",
    "    X_q1q2 = scipy.sparse.hstack((q1,q2))\n",
    "    \n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 27s\n",
      "Wall time: 4min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((323432, 9425768), (323432, 6), (80858, 6), (80858, 9425768))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time X_tr_q1q2 = get_features_from_df(train_df,count_vect)\n",
    "%time X_te_q1q2  = get_features_from_df(test_df, count_vect)\n",
    "\n",
    "X_tr_q1q2.shape, train_df.shape, test_df.shape, X_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./X_te_q1q2.npz', X_te_q1q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_train = train_df[\"is_duplicate\"].values\n",
    "y_test = test_df['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Trying with a simple model (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-513400f80517>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"liblinear\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr_q1q2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1542\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1543\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1544\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1545\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    940\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", verbose=1, max_iter=100)\n",
    "logistic.fit(X_tr_q1q2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logistic.score(X_tr_q1q2, y_train), logistic.score(X_te_q1q2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Improving results (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "N = 10000 # With early stopping\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=N)\n",
    "xgb_model.fit(X_tr_q1q2, y_train, \n",
    "              verbose=10, \n",
    "              eval_set=[(X_tr_q1q2, y_train),(X_te_q1q2, y_test)], \n",
    "              early_stopping_rounds =10,\n",
    "              eval_metric=['auc','logloss'],\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "results = xgb_model.evals_result()\n",
    "epochs = len(results['validation_0']['logloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "# plot log loss\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "ax.set_ylabel('Log Loss')\n",
    "ax.set_title('XGBoost Log Loss')\n",
    "\n",
    "# plot classification AUC\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "ax.legend()\n",
    "ax.set_ylabel('Classification AUC')\n",
    "ax.set_title('XGBoost Classification AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "xgb_model.save_model('models/model_count.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In our case, TFIDF has a lower performance than SimpleCountVectorizer (with the default parameters it already had it). We have managed to raise the score a little bit although we only use the CountVectorizer implemented at the beginning to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa840c0489fb4e54a914fd8f562fa67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building corpus: ', max=646864.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_vectorizer = TFIDFVectorizer(count_vect.vocabulary, count_vect.word_to_ind, count_vect.tokenize)\n",
    "tfidf_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_tfidf_tr_q1q2 = get_features_from_df(train_df, tfidf_vectorizer)\n",
    "X_tfidf_te_q1q2  = get_features_from_df(test_df, tfidf_vectorizer)\n",
    "\n",
    "X_tfidf_tr_q1q2.shape, train_df.shape, test_df.shape, X_tfidf_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", verbose=1, max_iter=1000)\n",
    "logistic.fit(X_tfidf_tr_q1q2, y_train)\n",
    "\n",
    "logistic.score(X_tfidf_tr_q1q2, y_train), logistic.score(X_tfidf_te_q1q2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 10000 # With early stopping\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=N)\n",
    "xgb_model.fit(X_tfidf_tr_q1q2, y_train, \n",
    "              verbose=10, \n",
    "              eval_set=[(X_tfidf_tr_q1q2, y_train),(X_tfidf_te_q1q2, y_test)], \n",
    "              early_stopping_rounds =10,\n",
    "              eval_metric=['auc','logloss'],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "xgb_model.save_model('models/model_tfidf.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Cosas importantes:\n",
    "He subido el notebook final 1: F1_Building_the_model\n",
    "He separado algunas funciones en una libreria utils.py (int2num, cast2int)\n",
    "He creado tambien una libreria mistakes.py con las cuatro funciones de mistakes. Quien escriba el notebook, que lo tenga en cuenta de no ponerlas en el notebook, solo importar \n",
    "\n",
    "from utils import *\n",
    "from mistakes import *\n",
    "\n",
    "He generado dos modelos finales model_count.dat, model_tfidf.dat, que pueden ser cargados y comparados con los de sklearn. El primero llega a una AUC de 88% i el segundo a 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
